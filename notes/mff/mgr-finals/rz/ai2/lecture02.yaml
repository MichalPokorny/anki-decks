deck: All::Magisterské státnice::Reprezentace znalostí::AI2::Přednáška 2
notes:
        # ai1 lecture02eng
        -       guid: IRg=`yi-Y%
                front: Struktura Bayesovské sítě
                back: |
                        Acyklický graf, hrana když existuje závislost,
                        ve vrcholech jsou podmíněné pravděpodobnostní
                        distribuce.
        -       guid: Ar@^=N+>`8
                front: |
                        Vzorec: Podmíněné distribuce Bayesovské sítě
                        &rarr; pravděpodobnost jednoho světa
                back: |
                        [$]P(x_1,\\ldots x_n)=
                        \\prod_i P(x_i | \\text{parents}(X_i))[/$]
        -       guid: I4Q|8qC/v<
                front: Podmíněné nezávislosti proměnných v Bayesovské síti
                back: |
                        [$]X[/$] je podmíněně nezávislá na proměnných, které
                        nejsou její potomci, pokud známe
                        [$]\\mathrm{parents}(X)[/$].
                        <br>
                        Proměnná je podmíněně nezávislá na všech ostatních
                        proměnných, pokud známe rodiče, potomky a rodiče potomků
                        (*Markov blanket*).
        -       guid: cKNn>g-F;q
                front: Základní algoritmus pro výpočet v Bayesovské síti
                back: |
                        Enumeration-Ask: dostane proměnnou a zafixovaná pozorování,
                        vrátí její distribuci.
                        Postupně vyzkouší všechny hodnoty a zavolá na nich Enumerate-All,
                        pak výsledky znormalizuje a vrátí distribuci.<br>
                        <br>
                        Enumerate-All: dostane proměnné (iniciálně všechny
                        v Bayesovské síti) a zafixovaná pozorování.
                        <br>
                        Když proměnné jsou prázdné, vrať 1.
                        <br>
                        Když první proměnná má dosazenou hodnotu,
                        vrať [$]P(Y=y|\\mathrm{Pa}(Y)) \\times \\mathrm{EnumAll}(\\mathrm{Rest}(vars),e)[/$].
                        <br>
                        Jinak vrať [$]\\sum_y P(Y=y|\\mathrm{Pa}(Y)) \\times \\mathrm{EnumAll}(\\mathrm{Rest}(vars),e+[Y=y])[/$].
        -       guid: b&LxV0e<)$
                topic: Bayesovské sítě
                front: Faktor, operace na faktorech
                back: |
                        Faktor: tabulka s proměnnými, hodnotami a pravděpodobnostmi.
                        Taky se jim říká "potenciály".
                        <br>
                        Násobení faktorů: spojím, pointwise.
                        <br>
                        Sum out (marginalizace): [$]\\sum_a f(A,B,C)=f(B,C)[/$]
        -       guid: Bg+mqnQmKs
                topic: Bayesovské sítě
                front: Inferenční algoritmus Eliminace proměnných, jeho složitost.
                back: |
                        Vstup: query proměnná, evidence, belief net.<br>

                        Faktory: [], proměnné: síť topologicky od příčin
                        k důsledkům<br>

                        Pro každou proměnnou:
                        přidej na začátek faktorů
                        [$]\\mathrm{Make-Factor}(var,e)[/$].
                        <br>
                        Jestli je proměnná skrytá, tak ji z faktorů vysčítej
                        (vymarginalizuj).
                        <br>
                        Vrať: normalizovaný pointwise produkt všech faktorů.
                        <br>
                        Složitost: velikost největšího faktoru
        -       guid: nt?![s@DL?
                topic: Bayesovské sítě
                front: Variable Elimination &mdash; jak vypadá operace sum-out?
                back: |
                        Sesumíruje dohromady všechny faktory, které obsahují
                        skrytou proměnnou &mdash; do jednoho velkého faktoru.
        -       guid: EnSVy1S>qi
                front: Variable Elimination &mdash; co ovlivní složitost?
                back: |
                        Pořadí eliminace proměnných.
                        Musí jít topologicky, ale kromě toho mám svobodu.
        -       guid: cO6Eu6HU]0
                front: |
                        Variable Elimination &mdash;
                        Složitost nalezení optimálního pořadí eliminace
                back: |
                        NP-těžké
        -       guid: NlI)a(6U-w
                front: |
                        Variable Elimination &mdash;
                        Heuristika nalezení další proměnné k eliminaci
                back: |
                        Vyber tu, co povede na vyrobení nejmenšího faktoru.
        -       guid: D?{V3`}@<d
                front: |
                        Složitost: Inference v Bayesovské síti co je les
                back: |
                        [$]\\O(n\\cdot d^k)[/$] čas i prostor (tj. stejně jako součet velikosti všech tabulek)
        -       guid: eD&r+!N#Q[
                front: |
                        Složitost: Inference v Bayesovské síti
                markdown: false
                back: |
                        [$]\sharp-\mathrm{P}[/$]-těžká (převodem z 3-SATu)
        -       guid: H6aMi5eN+T
                front: |
                        Algoritmy přibližné inference pro Bayesovské sítě
                back: |
                        Přímé vzorkování, Markov chain sampling
        -       guid: Ep+zE@vQ=d
                front: |
                        Přímé vzorkování v Bayesovských sítích,
                        rejection sampling
                back: |
                        Vezmu proměnné v topologickém pořadí, postupně
                        házej korunami podle pravděpodobností daných rodiči.
                        <br>
                        Rejection sampling: vygeneruju vzorek, mrknu jestli
                        sedí na pozorování. Když ne, zahodím ho. Jinak
                        stavím
                        [$]\\mathbf{P}(X|e)\\approx \\mathbf{N}(X,e)/N(e)[/$].
        -       guid: l%}C2*mg,W
                front: Likelihood weighting
                back: |
                        Nechci zahazovat vzorky jako v rejection samplingu.
                        <br>
                        Zafixuju evidence variables, sampluju jenom
                        non-evidence variables.
                        <br>
                        [$$]\\mathbf{P}(X|e)=\\alpha \\mathbf{N}(X,e) w(X,e)[/$$]
                        <br>
                        Kde: [$]w(X,e)=\\prod_j P(e_j|\\text{parents}(e_j))[/$]
                        <br>
                        (Bayesovo pravidlo. [$]P(z,e)=\\prod_i P(z_i|\\text{parents}(z_i))[/$])
                        <br>
                        Elegantně: když proměnná má přiřazenou viděnou hodnotu,
                        vynásob hmotnost vzorku pravděpodobností, že tuhle
                        hodnotu dostane. Když ji nemá, hoď ji náhodně podle
                        distribuce z jejich rodičů.
        -       guid: u*odI#Aj5Q
                topic: MCMC
                front: Zkratka
                back: Monte Carlo Markov chain
        -       guid: t>ddVP#=Kk
                topic: MCMC
                front: Jak funguje?
                back: |
                        Další vzorek dostanu tak, že vezmu nějakou proměnnou
                        co není v evidence a znova ji nasampluju podle její
                        Markovské obálky.
        -       guid: gxmJ5l]bAE
                topic: MCMC
                front: Proč funguje?
                back: |
                        Samplovací proces se ustálí v dynamickém ekvilibriu,
                        ve kterém dlouhodobě bude trávit ve stavech
                        poměr odpovídající jejich pravděpodobnosti.
                        <br>
                        Označ: [$]q(x\\rightarrow x')[/$] pravděpodobnost
                        přechodu, [$]\\pi_t(x)[/$] pravděpodobnost že jsem ve
                        stavu v čase [$]t[/$].
                        <br>
                        Obecně platí:
                        [$]\\pi_{t+1}(x')=\\sum_x \\pi_t(x)q(x\\rightarrow x')[/$].
                        <br>
                        Pro stacionární distribuci musí platit:
                        [$]\\pi(x')=\\sum_x \\pi(x) q(x\\rightarrow x')[/$].
                        <br>
                        Tohle platí například když [$]\\pi(x)q(x\\rightarrow x')=\\pi(x')q(x'\\rightarrow x)[/$].
                        <br>
                        Ať jsme změnili proměnnou [$]A[/$] z [$]a_i[/$] na [$]a'_i[/$]
                        a všechny ostatní ([$]B[/$] s hodnotami [$]b_i[/$]) nechali.<br>
                        [$$]q(x\\rightarrow x')=P(a'_i|b_i, e)=P(a'_i|mb(A_i))[/$$]
                        (Říká se tomu Gibbs sampling.)
                        <br>
                        [$$]\\pi(x) q(x\\rightarrow x')=
                        P(a_i, b_i|e) P(a_i'|b_i,e) =
                        [/$$]
                        [$$]=P(a_i|b_i,e) P(b_i|e) P(a_i'|b_i,e) =[/$$]
                        [$$]=P(a_i|b_i,e) P(a_i',b_i|e) = q(x'\\rightarrow x)\\pi(x')[/$$]
