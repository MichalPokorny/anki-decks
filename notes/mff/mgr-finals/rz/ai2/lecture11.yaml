deck: All::Magisterské státnice::Reprezentace znalostí::AI2::Přednáška 11
notes:
        # AI2 lecture11
        -       guid: B$oyJWw@{o
                topic: Reinforcement learning
                front: Odkud přijde reward?
                back: |
                        Reward je soucast perceptu.
        -       guid: IWnO9U=*[e
                topic: Reinforcement learning
                front: Reflex agent
                back: |
                        Naučí se policy, která mapuje ze stavů do akcí.
        -       guid: gXdU5HEkQT
                topic: Reinforcement learning
                front: Pasivní učení
                back: Učí se utility stavů, ale policy je zafixovaná.
        -       guid: p)PrmP6|_.
                topic: Reinforcement learning
                front: Aktivní učení
                back: Učí se utility stavů i policy, takže potřebuje i exploration
        -       guid: v-QaazI`E2
                front: |
                        Utility funkce, co se učí pasivní učení (nekonečná
                        suma)
                back: |
                        [$]U^\\pi(s)=\\mathbb{E}[\\sum_{t=0\\ldots\\infty} \\gamma^t \\cdot R(s_t)] [/$]
        -       guid: M<5@r)$Bqc
                topic: Reinforcement learning
                front: Co nezná pasivní učení?
                back: |
                        Agent nezná transition model [$]P(s'|s,a)[/$],
                        reward function [$]R(s)[/$] (postupně se učí z perceptů).
        -       guid: Hlv.3lBS{*
                topic: Reinforcement learning
                front: Direct utility estimation - jak funguje? Proč je neefektivní?
                back: |
                        Postupně sampluj pro každý stav jaké discountované utility jsme viděli.
                        Stane se z toho supervised learning.<br>

                        Neefektivita: Nepoužíváme Bellmanovy rovnice &mdash;
                        prohledávaný prostor
                        obsahuje fůru funkcí co je nesplňují a pomalu konvergujeme.
        -       guid: mncufF&g(x
                topic: Reinforcement learning
                front: Bellmanovy rovnice pro zafixovanou policy
                back: |
                        [$$]U^\\pi(s)=R(s)+\\gamma \\sum_{s'} P(s'|s,\\pi(s)) U^\\pi(s')[/$$]
        -       guid: Eal,]]{LIt
                topic: Reinforcement learning
                front: Adaptive dynamic programming - co se učí? K čemu?
                back: |
                        Učí se transition model [$]P(s'|s,\\pi(s))[/$],
                        odměny [$]R(s)[/$].<br>
                        Pomocí Bellmanových rovnic (například pomocí modified
                        policy iteration) se učí utilitu stavů.
        -       guid: D/t[;%~Q*Z
                topic: Reinforcement learning
                front: Pasivní ADP agent
                back: |
                        Když jsem ještě neviděl stav: řekni že initial utility je reward, [$]R[s]=r'[/$] reward signál
                        <br>
                        Jestli jsem přišel z jiného stavu, inkrementuj počet pozorovaných přechodů.
                        Updatuj tabulku viděných přechodů &mdash;
                        [$]P(t|s,a)\\leftarrow N_{s'|sa}[t,s,a]/N_{sa}[s,a][/$]
                        <br>
                        [$]U[/$] urči přes policy evaluation.
        -       guid: F;RCau_[Qs
                topic: Reinforcement learning
                front: (Passive) temporal-difference learning &mdash; update
                back: |
                        Používám viděné přechody na postupné updatování utilit.<br>

                        Update:
                        [$$]U^\\pi(s)\\leftarrow U^\\pi(s)+\\alpha\\cdot(R(s) + \\gamma U^\\pi(s') - U^\\pi(s))[/$$]
        -       guid: A{IjqeZFXL
                topic: Reinforcement learning
                front: Porovnání ADP a TD
                back: |
                        Temporal difference learning nepotřebuje transition model.
                        Na jeden observed transition udělá jeden update.<br>
                        Adaptive dynamic programming udělá bulk update.
                        Po každé iteraci chce konzistanci, takže když narazí na
                        překvapení, udělá rychlou propagaci.
        -       guid: G!kni]SoX(
                topic: Reinforcement learning
                front: Active reinforcement learning - forma Bellmanových rovnic co
                        by mělo splňovat ADP
                back: |
                        Vybírám optimální akci.
                        [$$]U^\\pi(s)=R(s)+\\gamma \\max_a \\sum_{s'} P(s'|s,a) U^\\pi(s')[/$$]
        -       guid: jbUu/&HD`^
                topic: Reinforcement learning
                front: Problémy s active reinforcement learningem přes maximalizaci
                        Bellmanových rovnic
                back: |
                        Je to greedy agent. Nenaučil se totiž, co všechno prostředí umožňuje.<br>
                        Musím udělat exploration/exploitation trade-off.
        -       guid: w$T]ZStdfV
                topic: Reinforcement learning
                front: Jak integrovat exploraci?
                back: |
                        <ul>
                        <li> *Občas náhodná akce*: extrémně pomalé.
                        <li> *Exploraci v odhadu utility*:
                        [$$]
                        U^+(s)\\leftarrow R(s)+\\gamma\\max_a f(\\sum_{s'} P(s'|s,a) U^+(s'), N(s,a))
                        [/$$]
                          <ul>
                          <li> [$]N(s,a)[/$]: kolikrát jsme viděli [$]s\\rightarrow_a[/$],
                          <li> [$]U^+[/$] optimistický odhad utility,
                          <li> [$]f(u,n)[/$] stoupá v [$]u[/$], klesá v [$]n[/$]
                          </ul>
                        </ul>
        -       guid: B|qyT}a_<Q
                topic: Reinforcement learning
                front: Q-learning &mdash; Q-funkce
                back: |
                        [$]Q(s,a)[/$]: hodnota akce [$]a[/$] ve stavu [$]s[/$].
                        Model-free.
        -       guid: fj~}#h_wVg
                topic: Reinforcement learning
                front: Vztah mezi Q funkcí a utilitou
                back: |
                        [$]U(s)=\\max_a Q(s,a)[/$]
        -       guid: Qzx~[sy6N;
                topic: Reinforcement learning
                front: Q-learning &mdash; rovnice která by měla platit postupně
                back: |
                        [$$]Q(s,a)=R(s)+\\gamma\\sum_{s'}P(s'|s,a)\\max_{a'}Q(s',a')[/$$]
                        Ale nebudu se na to učit explicitní model.
        -       guid: s%Zr9jViGZ
                topic: Reinforcement learning
                front: Q-learning &mdash; update
                back: |
                        Update se provede kdykoliv udělám akci [$]a[/$] a skončím tím ve stavu [$]s'[/$].<br>
                        [$$]Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma\\max_{a'}Q(s',a')-Q(s,a))[/$$]
                        <br>
                        Policy vyberu tak abych zase nějak podporoval exploraci.
        -       guid: Z}6k;rnQ5
                topic: Reinforcement learning
                front: SARSA &mdash; zkratka
                back: State-Action-Reward-State-Action
        -       guid: m<N4<]HB<8
                topic: Reinforcement learning
                front: SARSA &mdash; update rule, změna z Q-learning
                back: |
                        <ul>
                        <li> *SARSA*:
                        [$$]
                        Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma Q(s',a')-Q(s,a))
                        [/$$]
                        <br>
                        Provede se při každém [$]s,a,r,s',a'[/$] přechodu.
                        <li> *Q-learning*:
                        [$$]
                        Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma \\max_{a'} Q(s',a')-Q(s,a))
                        [/$$]
                        </ul>
        -       guid: ilW^n</f&A
                topic: Reinforcement learning
                front: SARSA vs. Q-learning
                back: |
                        Když agent je greedy, jsou identické.
                        <br>
                        Když exploruje, drobný rozdíl. Q-learning
                        nezajímá, jaká policy se doopravdy následuje.
                        Dokáže se naučit i když má adversarial/náhodnou
                        exploration policy.
                        <br>
                        SARSA se učí utilitu agenta, kterého řídí.
                        Funguje i když ostatní agenti částečně ovládají policy.
        -       guid: x/):uxEGgY
                topic: Reinforcement learning
                front: Rozdíly mezi Q-learningem a SARSOU vs. ADP
                back: |
                        Q-learning, SARSA se učí action-utility funkci
                        bez modelu.
                        Lokální updaty neenforcují konzistenci Q-hodnot.<br>
                        Intuice: čím složitější je prostředí, tím
                        užitečnější je dobrá reprezentace (ADP)
