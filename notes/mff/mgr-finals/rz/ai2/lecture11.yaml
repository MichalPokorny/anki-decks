deck: All::Magisterské státnice::Reprezentace znalostí::AI2::Přednáška 11
notes:
        # AI2 lecture11
        -
                uuid: 6f2dc87d-6cd2-4f0f-af24-41f59c1bd1e7
                topic: Reinforcement learning
                front: Odkud přijde reward?
                back: |
                        Reward je soucast perceptu.
        -
                uuid: 27ace965-3b57-42cd-bad0-d408486e7765
                topic: Reinforcement learning
                front: Reflex agent
                back: |
                        Naučí se policy, která mapuje ze stavů do akcí.
        -
                uuid: c9942fb6-b315-4fc1-b894-20c330b2e456
                topic: Reinforcement learning
                front: Pasivní učení
                back: Učí se utility stavů, ale policy je zafixovaná.
        -
                uuid: fdf84e9e-a1e8-4685-a6e4-bd5023208f91
                topic: Reinforcement learning
                front: Aktivní učení
                back: Učí se utility stavů i policy, takže potřebuje i exploration
        -
                uuid: 7f9a9722-e639-4a62-a0c5-197829ce21c6
                front: Utility funkce, kterou se učí pasivní učení; model (jako nekonečná suma)
                back: |
                        [$]U^\\pi(s)=\\mathbb{E}[\\sum_{t=0\\ldots\\infty} \\gamma^t \\cdot R(s_t)] [/$]
        -
                uuid: 2fb6e2af-125c-4dd4-8a86-3495a89ee87c
                topic: Reinforcement learning
                front: Co nezná pasivní učení?
                back: |
                        Agent nezná transition model [$]P(s'|s,a)[/$],
                        reward function [$]R(s)[/$] (postupně se učí z perceptů).
        -
                uuid: 105d7079-f317-4368-a9a3-f1541d832c30
                topic: Reinforcement learning
                front: Direct utility estimation - jak funguje? Proč je neefektivní?
                back: |
                        Postupně sampluj pro každý stav jaké discountované utility jsme viděli.
                        Stane se z toho supervised learning.<br>

                        Neefektivita: Nepoužíváme Bellmanovy rovnice &mdash;
                        prohledávaný prostor
                        obsahuje fůru funkcí co je nesplňují a pomalu konvergujeme.
        -
                uuid: 56c13553-e810-4ba1-9b36-d0e032c98581
                topic: Reinforcement learning
                front: Bellmanovy rovnice pro zafixovanou policy
                back: |
                        [$$]U^\\pi(s)=R(s)+\\gamma \\sum_{s'} P(s'|s,\\pi(s)) U^\\pi(s')[/$$]
        -
                uuid: 06a2032d-46dd-4da7-925c-711687a73d4e
                topic: Reinforcement learning
                front: Adaptive dynamic programming - co se učí? K čemu?
                back: |
                        Učí se transition model [$]P(s'|s,\\pi(s))[/$],
                        odměny [$]R(s)[/$].<br>
                        Pomocí Bellmanových rovnic (například pomocí modified
                        policy iteration) se učí utilitu stavů.
        -
                uuid: 1fdc283e-9b4e-4b5a-8ab2-a1ac6668df78
                topic: Reinforcement learning
                front: Pasivní ADP agent
                back: |
                        Když jsem ještě neviděl stav: řekni že initial utility je reward, [$]R[s]=r'[/$] reward signál
                        <br>
                        Jestli jsem přišel z jiného stavu, inkrementuj počet pozorovaných přechodů.
                        Updatuj tabulku viděných přechodů &mdash;
                        [$]P(t|s,a)\\leftarrow N_{s'|sa}[t,s,a]/N_{sa}[s,a][/$]
                        <br>
                        [$]U[/$] urči přes policy evaluation.
        -
                uuid: e42e5279-659a-48b2-9f03-831c0e7fc967
                topic: Reinforcement learning
                front: (Passive) temporal-difference learning &mdash; update
                back: |
                        Používám viděné přechody na postupné updatování utilit.<br>

                        Update:
                        [$$]U^\\pi(s)\\leftarrow U^\\pi(s)+\\alpha\\cdot(R(s) + \\gamma U^\\pi(s') - U^\\pi(s))[/$$]
        -
                uuid: 98b6eeca-65b2-4390-b02d-79299e42547d
                topic: Reinforcement learning
                front: Porovnání ADP a TD
                back: |
                        Temporal difference learning nepotřebuje transition model.
                        Na jeden observed transition udělá jeden update.<br>
                        Adaptive dynamic programming udělá bulk update.
                        Po každé iteraci chce konzistanci, takže když narazí na
                        překvapení, udělá rychlou propagaci.
        -
                uuid: 99f3d30a-3855-41b1-9704-ed6765005e04
                topic: Reinforcement learning
                front: Active reinforcement learning - forma Bellmanových rovnic co by mělo splňovat ADP
                back: |
                        Vybírám optimální akci.
                        [$$]U^\\pi(s)=R(s)+\\gamma \\max_a \\sum_{s'} P(s'|s,a) U^\\pi(s')[/$$]
        -
                uuid: 625edded-3b66-4b4c-9b28-330306392933
                topic: Reinforcement learning
                front: Problémy s active reinforcement learningem přes maximalizaci Bellmanových rovnic
                back: |
                        Je to greedy agent. Nenaučil se totiž, co všechno prostředí umožňuje.<br>
                        Musím udělat exploration/exploitation trade-off.
        -
                uuid: f33a95a2-8be5-4095-a52f-07a3b55d601f
                topic: Reinforcement learning
                front: Jak integrovat exploraci?
                back: |
                        <ul>
                        <li> *Občas náhodná akce*: extrémně pomalé.
                        <li> *Exploraci v odhadu utility*:
                        [$$]
                        U^+(s)\\leftarrow R(s)+\\gamma\\max_a f(\\sum_{s'} P(s'|s,a) U^+(s'), N(s,a))
                        [/$$]
                          <ul>
                          <li> [$]N(s,a)[/$]: kolikrát jsme viděli [$]s\\rightarrow_a[/$],
                          <li> [$]U^+[/$] optimistický odhad utility,
                          <li> [$]f(u,n)[/$] stoupá v [$]u[/$], klesá v [$]n[/$]
                          </ul>
                        </ul>
        -
                uuid: b7a2ae3f-3106-438d-8cc5-12b6b5879993
                topic: Reinforcement learning
                front: Q-learning &mdash; Q-funkce
                back: |
                        [$]Q(s,a)[/$]: hodnota akce [$]a[/$] ve stavu [$]s[/$].
                        Model-free.
        -
                uuid: 6363b605-6198-495b-ac6e-5f3f94607617
                topic: Reinforcement learning
                front: Vztah mezi Q funkcí a utilitou
                back: |
                        [$]U(s)=\\max_a Q(s,a)[/$]
        -
                uuid: 005c51b9-bad7-4633-b0e8-b09d403a7746
                topic: Reinforcement learning
                front: Q-learning &mdash; rovnice která by měla platit postupně
                back: |
                        [$$]Q(s,a)=R(s)+\\gamma\\sum_{s'}P(s'|s,a)\\max_{a'}Q(s',a')[/$$]
                        Ale nebudu se na to učit explicitní model.
        -
                uuid: 08559802-8f4c-4c41-ae11-6a2072109c0d
                topic: Reinforcement learning
                front: Q-learning &mdash; update
                back: |
                        Update se provede kdykoliv udělám akci [$]a[/$] a skončím tím ve stavu [$]s'[/$].<br>
                        [$$]Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma\\max_{a'}Q(s',a')-Q(s,a))[/$$]
                        <br>
                        Policy vyberu tak abych zase nějak podporoval exploraci.
        -
                uuid: 80a8a8f3-3537-4911-8e9d-8b5f38553cfe
                topic: Reinforcement learning
                front: SARSA &mdash; zkratka
                back: State-Action-Reward-State-Action
        -
                uuid: 6ea21443-2301-44b6-ae1f-55cc2f09725f
                topic: Reinforcement learning
                front: SARSA &mdash; update rule, změna z Q-learning
                back: |
                        <ul>
                        <li> *SARSA*:
                        [$$]
                        Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma Q(s',a')-Q(s,a))
                        [/$$]
                        <br>
                        Provede se při každém [$]s,a,r,s',a'[/$] přechodu.
                        <li> *Q-learning*:
                        [$$]
                        Q(s,a)\\leftarrow Q(s,a)+\\alpha(R(s)+\\gamma \\max_{a'} Q(s',a')-Q(s,a))
                        [/$$]
                        </ul>
        -
                uuid: 15ee1ee2-e64d-41f8-98ae-0fbd136a0b53
                topic: Reinforcement learning
                front: SARSA vs. Q-learning
                back: |
                        Když agent je greedy, jsou identické.
                        <br>
                        Když exploruje, drobný rozdíl. Q-learning
                        nezajímá, jaká policy se doopravdy následuje.
                        Dokáže se naučit i když má adversarial/náhodnou
                        exploration policy.
                        <br>
                        SARSA se učí utilitu agenta, kterého řídí.
                        Funguje i když ostatní agenti částečně ovládají policy.
        -
                uuid: 38101815-5bf6-4285-b98f-1001cf9f9e0b
                topic: Reinforcement learning
                front: Rozdíly mezi Q-learningem a SARSOU vs. ADP
                back: |
                        Q-learning, SARSA se učí action-utility funkci
                        bez modelu.
                        Lokální updaty neenforcují konzistenci Q-hodnot.<br>
                        Intuice: čím složitější je prostředí, tím
                        užitečnější je dobrá reprezentace (ADP)
