deck: All::Magisterské státnice::Reprezentace znalostí::AI2::Přednáška 6
notes:
        # AI2 lecture06eng
        -       guid: v,iq*b!)_&
                front: Decision network
                back: |
                        Node:
                        <ul>
                        <li> *Chance* [$]\\circ[/$]: náhodné proměnné (jako v
                        Bayesovských sítích)
                        <li> *Decision* [$]\\square[/$]: kde decision maker
                        dělá rozhodnutí
                        <li> *Utility* [$]\\diamond[/$]: utilita
                        </ul>
        -       guid: AP5ZA6p(;-
                front: Sensitivity analysis
                back: |
                        Zjistit, co se stane, když trochu tweaknu hodnoty co
                        jsou v (Bayesovském) modelu. (Neměl bych mít příliš
                        citlivý.)
        -       guid: chvC12RT!v
                front: Markovian state transition model
                back: |
                        Pevné pravděpodobnosti [$]P(s'|s,a)[/$].
        -       guid: AX]6_ksGmi
                front: Markov decision process
                back: |
                        Sequential decision problem for a fully observable,
                        stochastic environment with a Markovian transition
                        model and additive rewards.
        -       guid: rANTBi8YJU
                front: What's the solution to a MDP?
                back: |
                        A policy ([$]\\pi(s)[/$]).
                        Optimal policy: highest expected utility.
        -       guid: P;&gC^<o26
                front: How to deal with utilities over time?
                back: |
                        <ol type="A">
                        <li> Finite horizon; after some time, nothing matters.
                        <li> Infinite horizon: optimal policy is stationary.
                        </ol>
                        Rewards: additive or discounted.
                        Discounting: exponential.
        -       guid: JlO+*-ce/z
                front: Expected utility from policy and reward function
                back: |
                        [$$]U^\\pi(s)=E[\\sum_{i=0\\ldots\\infty} \\gamma^i R(S_i)][/$$]
        -       guid: fiJLPl1}}c
                front: Definition - true utility of state; how to go from that to
                        best policy
                back: |
                        [$]U(s)=U^{\\pi^{ * }}(s)[/$]

                        Optimal policy:
                        [$]\\pi^{ * }(s)=\\arg\\max_a \\sum_{s'}P(s'|s,a)U(s')[/$]
        -       guid: JDhbUz-{ub
                front: Bellman equation for true utilities
                back: |
                        [$$]U(s)=R(s)+\\gamma\\max_{a}\\sum_{s'}P(s'|s,a)U(s')[/$$]
        -       guid: P2{)H?IEFP
                front: Value iteration algorithm, kdy se skončí?
                back: |
                        Vstup: [$]\\epsilon[/$]: jak daleko si dovolím být od
                        skutečných utilit.
                        Potřebuju přechodový model.
                        <br><br>
                        Začni s náhodnými počátečními hodnotami.
                        Opakuj Bellmanovský update:
                        [$]
                        U_{i+1}(s)\\leftarrow R(s)+\\gamma\\max_a \\sum_{s'}P(s'|s,a)U_i(s')
                        [/$]
                        Konči, až se utilita změní jen málo:
                        [$]\\delta < \\varepsilon (1-\\gamma)/\\gamma[/$].
        # TODO: konvergence value iteration
        -       guid: HDq2l^<4os
                front: Definice - policy loss
                back: |
                        [$$]|U^{\\pi} - U|_{\\max}[/$$]
        -       guid: bIWtpt7`;.
                front: Maximální policy loss po [$]i[/$] krocích value iteration
                back: |
                        [$$]|U_i-U| < \\varepsilon \\rightarrow |U^{\\pi_i}-U| < 2\\varepsilon\\gamma(1-\\gamma)[/$$]
        -       guid: o>|10B`j},
                topic: Policy iteration
                front: Motivace proti value iteration
                back: |
                        Value iteration policy často zkonverguje daleko dřív než
                        zkonvergují utility. Nezáleží na přesném rozdílu když
                        je jedna akce jasně optimální.
        -       guid: m09L)2g[gf
                topic: Policy iteration
                front: Algoritmus
                back: |
                        Opakuje kroky:
                        <ol type="1">
                        <li> Policy evaluation: Počítám utilitu.
                        (Pro malé stavové prostory jde přesně v
                        [$]\\O(n^3)[/$]; znám přechodový model).
                        <br>
                        Pro velké prostory: několik kroků value iteration.
                        <li> Policy improvement: vyberu pro stavy optimální
                        akce podle současných utilit.
                        </ul>
                        Pokud jsem změnil policy, opakuju.
        -       guid: L.V%w/Dog<
                front: Model rozhodování v MDP vs. POMDP - co se přidá?
                back: Prostředí je jenom částečně pozorovatelné
        -       guid: P=juSG!!)w
                front: Model POMDP
                back: |
                        Máme [$]P(s'|s,a)[/$], reward function [$]R(s)[/$],
                        sensor model [$]P(e|s)[/$].
                        <br>
                        Neznáme náš stav.
        -       guid: r4Gk]M=<r[
                topic: POMDP
                front: |
                        Jak reprezentovat neznámé stavy a konceptuálně hledat
                        správné akce?
                back: |
                        Neznámý stav aproximujeme jako belief stav &mdash; vektor
                        pravděpodobností, že jsem v nějakém stavu.
                        Budu hledat optimální policy pro belief staty:
                        [$]\\pi(b)[/$], kde [$]b(s)\\in[0;1][/$].
        -       guid: h0*]%8|d`*
                topic: POMDP
                front: Jak spočítat další belief, když máme pozorování?
                back: |
                        [$]b'(s')=\\alpha P(e|s') \\sum_s P(s'|s,a) b(s)[/$];
                        to je to samé jako operace FORWARD
        -       guid: wgexjt6PFl
                topic: POMDP
                front: Jak spočítat další belief, když nemáme pozorování?
                back: |
                        Když v belief statu udělám [$]a[/$], pravděpodobnost že
                        pak dostanu [$]e[/$]:
                        [$$]P(e|a,b) = \\sum_{s'} P(e|a,b,s') P(s'|a,b) = \\sum_{s'} P(e|s') P(s'|a,b) = \\sum_{s'} P(e|s') \\sum_{s} P(s'|a,s) b(s)[/$$]
                        <br>
                        Pravděpodobnost, že se dostanu z [$]b[/$] do [$]b'[/$]:
                        [$$]P(b'|b,a)=\\sum_e P(b'|a,b,e) P(e|a,b) = \\sum_e P(b'|a,b,e) \\sum_{s'} P(e|s') \\sum_{s} P(s'|a,s) b(s)[/$$]
                        <br>
                        Kde: [$]P(b'|a,b,e)=1[/$] když [$]b=FORWARD(b,a,e)[/$]
        -       guid: JeCjgqawZJ
                topic: POMDP
                front: Zobecnění reward function pro POMDP belief state
                back: |
                        [$$]r(b)=\\sum_s b(s) R(s)[/$$]
        -       guid: E^aN>;j.x+
                topic: POMDP
                front: Redukce POMDP na MDP
                back: |
                        [$]P(b'|b,a)[/$] a [$]r(b)[/$] (na belief statech)
                        jsou pozorovatelný MDP. Akorát že z toho dostanu
                        spojitý MDP...
        -       guid: K`|168E,3z
                topic: POMDP
                front: Architektura online agentů pro POMDP
                back: |
                        Podmíněné plány: akce se vybere podle pozorování a
                        předchozích akcí.
                        <br>
                        Value iteration se dá modifikovat na POMDP, ale
                        není moc efektivní.
                        <br>
                        Použijeme dynamické Bayesovské sítě a look-ahead
                        techniky.
        -       guid: H9$A^QH,z(
                front: Podobnost řešení POMDP a hraní her
                back: |
                        Nejistota je jako protihráč. Můžeme použít
                        něco jako EXPECTEDMINIMAX.
                        Hloubku stromu zjistíme podle discount faktoru
                        (jakmile je hodně malá změna podle toho co tam uděláme,
                        neprodlužujeme).
