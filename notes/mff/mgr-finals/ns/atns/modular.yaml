deck: All::Magisterské státnice::Neuronové sítě::Aplikace teorie neuronových sítí::Modulární
        sítě
notes:
        # ATNS modular
        -       guid: e%Ho?2Tq5b
                front: 2 druhy robustnosti NS a jak se jich dosahuje
                back: |
                        Vzhledem ke ztrátě neuronu: přidám robustnostní člen
                        do chybové funkce; prořezání a duplikace: zdvojení těch
                        neuronů, jejichž ztráta způsobí největší odchylku na
                        výstupu sítě<br>

                        Vzhledem k drobným odchylkám vstupu: zkonstruuj
                        [$]\\varepsilon[/$]-ekvivalentní síť co je robustnější
        -       guid: vzNU%Fy:V%
                front: |
                        [$]\\varepsilon[/$]-ekvivalence neuronových sítí
                back: |
                        Jsou [$]\\varepsilon[/$]-ekvivalentní, když pro všechny vstupy [$]\\|y_{B_1} - y_{B_2}\\|\\leq\\varepsilon[/$]
        -       guid: qGw1#SUD#V
                front: Robustnostní chybová funkce &mdash; různé varianty, jejich
                        různé následky
                back: |
                        [$]y_{i,-k}[/$]: výstup [$]i[/$] když zruším neuron [$]k[/$]<br>

                        [$]\\varepsilon = E_{d_m} - E_1 =
                        \\frac{1}{2N_h}\\sum_p \\sum_i \\sum_k (y_{i, -k}-d_i)^2 - \\frac{1}{2}\\sum_p \\sum_i (y_i-d_i)^2[/$]:
                                chyba když zmizí průměrný skrytý neuron<br>

                        Odchylka výstupu ke ztrátě neuronu: [$]E_{y_m}=\\frac{1}{2N_h}\\sum_p\\sum_i\\sum_k(y_i-y_{i,-k})^2[/$]<br>

                        Nejhorší následky: [$]E_d=\\max_{p,i,k}\\|y_{i,-k}-d_i\\|[/$]
                        [$]E_y=\\max_{p,i,k}\\|y_{i,-k}-y_i\\|[/$]
        -       guid: v~e8g:5%JP
                markdown: false
                front: Trik při minimalizaci robustnostního členu [$]E_{y_m}[/$]
                back: |
                        Minimalizuj místo toho
                        [$]E_{y'_m}=\frac{1}{N_h}\sum_p\sum_i\sum_k(\xi_i-\xi_{i,-k})^2=
                        \frac{1}{2N_h}\sum_p\sum_i\sum_j(x_j w_{ij})^2[/$]
                        ([$]\xi_i[/$] místo [$]y_i[/$])
        -       guid: xhuSVu^lTb
                front: Prořezávání a duplikace
                back: |
                        Adaptuj síť přes backprop.
                        Identifikuj neuron, jehož ztráta by způsobila největší změnu robustnosti.
                        "Zdvoj" ho.
                        "Rozpůl" váhy obou "kopií" vedoucí k výstupní vrstvě.
                        "Kopie" nahradí některý z ostatních skrytých neuronů.<br>

                        Výsledek jsou robustnější sítě, malá výpočetní složitost, lepší generalizace, omezení problémů s lokálními minimy.
        -       guid: jLcZP@?fYh
                front: Co je to modul BP-sítě?
                back: |
                        Stejný počet vrstev, každá skrytá vrstva je podmnožina skryté vrstvy hlavní sítě.
        -       guid: kC!HS@(jr;
                front: Co chceme od NS s modulární architekturou?
                back: |
                        Rychlost a konvergenci učení.<br>
                        Optimalizaci architektury.<br>
                        Robustnost a generalizaci.<br>
                        "Useful diversity" jednotlivých modulů.
        -       guid: K8^p*{,.)M
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Architektura
                back: |
                        Soustava lokálních sítí s řídící sítí.<br>
                        Lokální jsou BP. Všechny mají stejnou topologii.<br>
                        Řídící síť má výstupy [$]P_j[/$], stejné vstupy jako lokální sítě.<br>
                        Výstup je [$]\\sum_j P_j y^{(j)}[/$].
        -       guid: I*.DXTEYZ[
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Co by mělo platit pro koeficienty [$]p_i[/$]?
                back: |
                        [$$]p_i=\\frac{y_i^{(g)}}{\\sum_{j=1}^t y_j^{(g)}}, i=1\\ldots t, p_i\\in(0;1), \\sum_i^t p_i = 1[/$$]
        -       guid: GuM*1zf@;z
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Chybová funkce
                back: |
                        Pro jeden vzor [$]p[/$]:
                        [$$]E_p=\\frac{1}{2}\\sum_{i=1}^t (p_i\\|y^{(i)}-d\\|)^2[/$$]<br>

                        Plná chybová funkce je suma přes všechny vzory.<br>

                        Kde: [$]p_i[/$] je gating koeficient pro danou síť,
                        [$]y^{(i)}[/$] je výstup jedné sítě, [$]t[/$] je počet
                        síťových modulů.<br>

                        Vyjádří se jako:
                        [$$]E_p = \\frac{1}{2}\\sum_{i=1}^t (\\sum_k (g_{k}^{(i)})^2)[/$$]
                        Kde [$]g_k^{(i)}=p_i\\|y_k^{(i)}-d_k\\|[/$] pro výstupní neuron, jinak [$]0[/$]
        -       guid: MPgZYp8;WC
                front: Síť na inverzní kinematiku robota
                back: |
                        Kontextová síť: na základě kontextového vstupu určuje váhy pro funkční síť.<br>
                        Převedení: zobrazení [$]n\\rightarrow n^2[/$] na [$]n^2[/$] funkcí [$]n\\rightarrow 1[/$]

