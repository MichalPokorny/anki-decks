deck: All::Magisterské státnice::Neuronové sítě::Přednáška 3 (vrstevnaté sítě)
notes:
        # 03-multilayered-nns
        -       guid: ucTMBvM0vF
                front: Chybová funkce pro feedforward sítě
                back: |
                        [$]E=\\frac{1}{2}\\sum_{p} \\sum_{j} (y_{j,p}-d_{j,p})^2[/$],
                        kde [$]j[/$] je index ve výstupním vektoru, [$]p[/$] je vzor
        -       guid: k%G_fU[oZW
                front: Jak vypadá update pro feedforward sítě, obecně?
                back: |
                        [$$]
                        w_{ij} -= \\frac{\\partial E}{\\partial w_{ij}} =
                        \\frac{\\partial E}{\\partial y_j}\\frac{\\partial y_j}{\\partial \\xi_j}\\frac{\\partial \\xi_j}{\\partial w_{ij}}
                        [/$$]
        -       guid: g}hh%U1CYI
                front: Co je v updatu pro feedforward sítě [$]\\delta[/$]?
                back: |
                        [$]
                        \\delta_j\\equiv-\\frac{\\partial E}{\\partial \\xi_j}
                        [/$]
        -       guid: b:<N$y~LBv
                markdown: false
                front: Jak vypadá použití [$]\delta[/$] v updatovacím pravidle?
                back: |
                        [$]
                        w_{ij}+=\delta_j\frac{\partial\xi_j}{\partial w_{ij}}=\delta_j y_i
                        [/$]
        -       guid: zuaet0&@&l
                front: Plná forma updatu - váhy do výstupní vrstvy
                back: |
                        [$]w_{ij}-=(y_j-d_j)f'(\\xi_j)y_i[/$]
        -       guid: eH*m2,N{O=
                front: Plná forma updatu - váhy do skrytých vrstev
                markdown: false
                back: |
                        [latex]
                        \begin{align*}
                        w_{ij} & -= & (\sum_k \frac{\partial E}{\partial \xi_k} w_{jk}) f'(\xi_j) y_i = \\
                               &  = & (\sum_k \delta_k w_{jk}) f'(\xi_j) y_i
                        \end{align*}
                        [/latex]
        -       guid: o(x5htV,m(
                front: |
                        Derivace sigmoidy s parametrem [$]\\lambda[/$] vzhledem
                        k její hodnotě
                back: |
                        [$]\\sigma'(x) = \\lambda \\sigma(x) (1-\\sigma(x))[/$]
        -       guid: f|LbLKv7<&
                front: Aktualizace vah s momentem
                back: |
                        [$]
                        w_{ij}^{(t+1)} = w_{ij}^{(t)} + \\alpha \\delta_j y_i +
                                \\alpha_m (w_{ij}^{(t)} - w_{ij}^{(t-1)})
                        [/$]
        -       guid: pj9K]*SlFq
                front: Velikost momentu učení
                back: |
                        [$]\\alpha_m \\in [0;1)[/$]
        -       guid: mzYK&pnD~|
                front: Složitost učení neuronových sítí
                back: NP-úplný
        -       guid: LT.U)]!vzH
                front: Chytrá volba počátečních vah - proč nulově vycentrované?
                back: |
                        Budou v intervalu [$][-\\alpha_m;\\alpha_m][/$].
                        Protože přenosová funkce má v 0 maximální derivaci.
                        Šířená chyba je pak větší.
        -       guid: NvFwkSXt#r
                front: Proč jsou problém příliš velké i příliš malé váhy?
                back: |
                        Malé: Moc malá šířená chyba.<br>
                        Velké: Chybová funkce v saturované zóně,
                        je plochá.
        -       guid: jH$%VsH$Ry
                topic: Chytrá volba vah
                front: Normalizace vstupů
                back: |
                        Do [$][0;1][/$].
        -       guid: bU=zsL-VCZ
                topic: Chytrá volba vah
                front: |
                        Střední hodnota potenciálu když nainicializujeme váhy
                        náhodně v [$]0\\pm\\alpha_m[/$]
                back: |
                        [$$]
                        E[\\xi_j] =
                        E[\\sum_{i=0}^n w_{ij} x_i]=
                        \\sum_{i=0}^n E[w_{ij}] E[x_i] = 0
                        [/$$]
        -       guid: mZ.6m3M8]E
                topic: Chytrá volba vah
                front: Odvození rozptylu potenciálu
                markdown: false
                back: |
                        [latex]
                        \begin{align*}
                        \sigma_{\xi_j}^2 & = & E[ (\xi_j)^2] - (E[\xi_j])^2 = E[ (\sum_{i=0}^n w_{ij} x_i)^2] - 0 = \\
                                         & = & \sum_{i,k=0}^{n} E[w_{ij} w_{kj} x_i x_k] = \sum_{i=0}^n E[(w_{ij})^2] E[(x_i)^2]
                        \end{align*}
                        [/latex]
        -       guid: ffSgWB&d[C
                topic: Chytrá volba vah
                front: |
                        [$] \\mathbb{E}[(x_i)^2][/$] když je [$]x[/$]
                        rovnoměrně v [$][0;1][/$]?
                back: |
                        [$]1/3[/$]
        -       guid: w%:fglQSS
                topic: Chytrá volba vah
                front: |
                        [$] \\mathbb{E}[(w_{ij})^2][/$] když je [$]w_{ij}[/$]
                        rovnoměrně v [$][-a;a][/$]?
                back: |
                        [$]a^2/3[/$]
        -       guid: Ay4d<p>()*
                topic: Chytrá volba vah
                front: |
                        Když chci aby měl [$]\\xi[/$] směrodatnou odchylku
                        [$]A=\\sigma_{\\xi}[/$], jak zvolit interval
                        [$][-a;a][/$] pro vstupní váhy?
                        Jaká je chytrá volba [$]A[/$]?
                back: |
                        [$]a = 3A/\\sqrt{N}[/$]
                        ([$]A=\\sigma_\\xi = a/3 \\sqrt{N}[/$]).
                        Chytrá volba: [$]A=1[/$] (rychlé učení, velký gradient)
        -       guid: Mh+n^i$M=7
                front: K čemu je moment?
                back: Snižuje oscilaci, tedy urychluje učení
        -       guid: j6Efi_=&Q.
                topic: Adaptivní parametr učení
                front: Obecně
                back: Pro každou váhu samostatný parametr [$]\\alpha_{ij}[/$]
        -       guid: x[bChapLj;
                topic: Adaptivní parametr učení
                front: Silva-Almeida heuristika
                back: |
                        Nezměnilo se znaménko parciální derivace &rarr;
                        zrychli ([$]\\cdot \\uparrow[/$]).
                        Změnilo se &rarr; zpomal ([$]\\cdot\\downarrow[/$]).
        -       guid: 'ftU5f,7>[:'
                topic: Adaptivní parametr učení
                markdown: false
                front: |
                        Delta-bar-delta algoritmus, proč je lepší
                        než Silva-Almeida
                back: |
                        Není tak citlivý na exponenciální růst když se dlouho
                        zrychluje.
                        <br><br>

                        [$]\delta_{i}^{(k)} =
                        (1-\Phi)\frac{\partial E}{\partial\xi_{i}}+\Phi\delta_i^{(k+1)}[/$],
                        kde [$]\Phi[/$] je konstanta.
                        <br><br>
                        Když [$]\delta_{i}^{(k-1)}\cdot\frac{\partial E}{\partial\xi_{i}}>0[/$],
                        tak [$]\alpha_{i}+=\uparrow[/$],
                        jinak [$]/=\downarrow[/$].
                        <br><br>
                        Aktualizace vah bez momentu:
                        [$]w_i -= \alpha_i^{(k)} \delta_i^{(k)} y_i^{(k)}[/$]
        -       guid: q5WAC:H.f]
                topic: Adaptivní parametr učení
                front: Super-SAB algoritmus
                back: |
                        <ul>
                        <li> Backprop s momentem
                        <li> Začni s rychlostí 1.2
                        <li> Znaméno se nezměnilo &rarr; vynásob rychlost 1.05
                        <li> Změnilo se &rarr; undoni poslední změnu, poděl
                             rychlost 2, pro účely momentu v dalším updatu se
                             tvař že změna v téhle souřadnici byla 0.
                        </ul>
        -       guid: g=jf@&=Irr
                front: Taylorova řada pro aproximaci [$]E[/$] druhého řádu
                back: |
                        [$]E(w+h)\\approx
                        E(w)+\\nabla E(w)^T\\cdot h+\\frac{1}{2}h^T \\nabla^2 E(w) h[/$]
        -       guid: IQP~1@vmB/
                front: |
                        Aproximace gradientu druhého řádu.
                        O co se s tím snažíme?
                back: |
                        [$]\\nabla E(w+h)^T\\approx\\nabla E(w)^T + h^T \\nabla^2 E(w)[/$]
                        <br>
                        Snažíme se gradient
                        [$]h = -(\\nabla^2 E(w))^{-1} \\nabla E(w)[/$]
                        mít 0 (hledáme [$]\\min E[/$]).
        -       guid: B_b2tyl4LQ
                front: Newtonovské metody pro druhý řád a problémy
                back: |
                        Aktualizace vah:
                        [$]w_{ij} -= (\\nabla^2 E(w))^{-1} \\nabla E(w)[/$]
                        <br>
                        Problém: výpočet inverzní Hessovské matice
        -       guid: yO}:Auzttb
                topic: Pseudonewtonovské metody
                front: |
                        Jak zjednodušují newtonovské metody
                back: |
                        Berou v úvahu jenom diagonálu Hessovské matice.
        -       guid: f>=oEfe.z{
                topic: Pseudonewtonovské metody
                front: |
                        Adaptační vzoreček
                back: |
                        [$]
                        w_{ij}-=\\frac{\\nabla_{ij}E(w)}{\\frac{\\partial^2 E(w)}{\\partial w_{ij}^2}}
                        [/$]
        -       guid: v($Fn<*4b6
                topic: Pseudonewtonovské metody
                front: |
                        Kdy dobře fungují
                back: |
                        "Kvadratický tvar" chybové funkce
        -       guid: Lsu~CCVFe=
                topic: Algoritmus QuickProp
                front: Alternativní způsob psaní aktualizace. Čemu odpovídá jmenovatel?
                markdown: false
                back: |
                        [$]w_{ij}+=\Delta^{(k)}w_i[/$]<br>

                        [$]\Delta^{(k)}w_i=\Delta^{(k-1)}w_i\cdot\frac{\nabla_i E^{(k)}}{\nabla_i E^{(k-1)}-\nabla_i E^{(k)}}[/$]<br>

                        Jde psát taky:
                        [$]
                        \Delta^{(k)}w_i=-\frac{\nabla_i E^{(k)}}{\frac{\nabla_i E^{(k)}-\nabla_i E^{(k-1)}}{\Delta^{(k-1)}w_i}}
                        [/$]<br>

                        Jmenovatel: diskrétní aproximace [$]\partial^2 E(w)/\partial w_i^2[/$]
        -       guid: HjwsRi|sC(
                front: Levenberg-Marquardtův algoritmus
                back: |
                        Je rychlejší a přesnější v oblasti minima [$]E[/$].
                        Kombinace gradientní a Newtonovy metody.
                        [$]w_{\\min}=w_0-(H+\\lambda I)^{-1} \\cdot g[/$], kde [$]g[/$] je gradient, [$]H[/$] je Hessovská matice<br>

                        Pro 1 výstup: [$]g_i = \\partial E/\\partial w_i = 2(y-d)\\frac{\\partial y}{\\partial w_i}[/$]<br>

                        [$$]\\partial^2 E/\\partial w_i \\partial w_j = 2[\\frac{\\partial y}{\\partial w_i}\\frac{\\partial y}{\\partial w_j} + (y-d)\\frac{\\partial^2 y}{\\partial w_i \\partial w_j}][/$$]<br>
                        Druhý člen &mdash; ten s [$] (y-d)[/$] &mdash; se zanedbá.
        -       guid: hX-)+fD+>i
                front: Relaxační metody - perturbace vah
                back: |
                        Pro náhodnou váhu spočítej chybovou funkci když se
                        trochu posune, podle spočítej diskrétní aproximaci
                        gradientu, aktualizuj.
        -       guid: yZfye4Mf1D
                front: Alternativní perturbační metoda s rychlejší konvergencí
                back: |
                        Zperturbuj [$]o_i[/$] - výstup neuronu [$]i[/$] -
                        o [$]\\Delta o_i[/$].
                        Spočítej rozdíl chyb. Zlepší se &rarr; spočítej,
                        jak má vypadat potenciál, aby měl neuron tenhle
                        výstup. Adaptuj váhy proporcionálně k velikosti:
                        [$]w_i'/\\xi'=w_i/\\xi[/$].
