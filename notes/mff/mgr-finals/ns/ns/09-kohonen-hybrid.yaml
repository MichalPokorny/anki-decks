deck: All::Magisterské státnice::Neuronové sítě::Přednáška 9 (Kohonenovy sítě a hybridní
        sítě)
notes:
        # 09-kohonen-hybrid
        -       guid: mK^ZpD(v_0
                topic: Kononenova mapa
                front: |
                        Účel, parametry
                back: |
                        Učení bez učitele.<br>
                        Najdi topologické okolí nejbližšího neuronu a taky ho
                        uprav.<br>
                        Funkce laterální interakce: [$]\\Phi(i,k)[/$],
                        například "funkce mexického klobouku"<br>
                        Vigilační koeficient [$]\\alpha\\in(0;1)[/$] určuje plasticitu, klesá v čase.
        -       guid: d>79`11:kd
                topic: Kononenova mapa
                front: Co by mělo platit pro vigilační koeficient?
                markdown: false
                back: |
                        [$]\sum_{t=1}^\infty \alpha(t)=\infty, \sum_{t=1}^\infty \alpha^2(t) < \infty[/$]
        -       guid: QX^{xtHj=U
                topic: Kononenova mapa
                front: |
                        Kostra důkazu: Jeden neuron Kohonenovy mapy na intervalu
                        [$][a;b][/$] se ustálí ve středu
                back: |
                        Adaptační pravidlo: [$]w_n\\leftarrow w_{n-1}+\\eta(x-w_{n-1})[/$],
                        kde [$]x[/$] je náhodně zvolené z [$][a,b][/$].
                        Pro [$]\\eta\\in(0;1)[/$] nemůže [$]w[/$] opustit [$][a,b][/$]<br>

                        Ocekavana hodnota [$]\\mathbb{E}[dx/dt][/$] musi byt 0,
                        jinak by mohlo byt [$]\\mathbb{E}[x]\\notin [a;b][/$].<br>

                        [$$]\\mathbb{E}[dx/dt]=\\eta(\\mathbb{E}[x]-\\mathbb{E}[w])=
                        \\eta((a+b)/2 - \\mathbb{E}[x]) = 0 \\longrightarrow \\mathbb{E}[w]=(a+b)/2[/$$]
        -       guid: f&VzJhJPQ[
                topic: Kononenova mapa
                front: Varianty učících algoritmů pro učení s učitelem pro Kohonenovy
                        mapy
                back: |
                        <ul>
                        <li>
                        LVQ1: [$]x[/$] by mělo patřit ke stejné třídě jako
                        nejbližší [$]w_i[/$].<br>

                        Vezmi nejbližší neuron, když je klasifikovaný stejně,
                        přibliž ho, kryž různě, oddal ho. S ostatními neurony
                        nehýbej.
                        <li>
                        LVQ2.1: Adaptuj 2 nejbližší sousedy současně. Jeden
                        z nich musí patřit ke správné třídě, druhý k nesprávné.
                        Když je [$]x[/$] z okenka mezi [$]w_i[/$], [$]w_j[/$] a
                        ty neurony maji ruzne tridy, tak spravny neuron prisun
                        a spatny odsun.<br>
                        Okenko: [$]\\min(\\frac{d_i}{d_j}, \\frac{d_j}{d_i})>\\frac{1-s}{1+s}[/$],
                        kde [$]s[/$] je sirka okenka, treba [$]0.2[/$] az [$]0.3[/$]
                        <li>
                        LVQ3: Plus: jestlize [$]w_i[/$] i [$]w_j[/$] patri do
                        stejne tridy, tak je prisun
                        o [$]\\varepsilon \\alpha(t)[x(t)-w_{i/j}(t)][/$], kde [$]0.1\\leq\\varepsilon\\leq 0.5[/$]
                        </ul>
        -       guid: uu;gP=ReQO
                front: |
                        Jméno: Vrstva nad Kohonenovskou
                        v counterpropagation sítích
                back: Grossbergovská
        -       guid: M5`^nui/<y
                front: Sítě se vstřícným šířením (counterpropagation) - architektura,
                        učení
                back: |
                        2 vrstvy: Kohonenovská, Grossbergovská (adaptuje váhy
                        jen pro vítěze z Kohonenovy vrstvy).
                        Použití: heteroasociativní paměť.<br>

                        <ol type="1">
                        <li> Vyber nejblizsi neuron v Kohonenovské vrstvě
                        <li> Aktualizuj vahy v Kohonenovske vrstve
                        <li> Uprav vahy mezi "vitezem" [$]c[/$] z Kohonenovy
                             vrstvy a neurony Grossbergovske vrstvy, aby vystup
                             odpovidal vic ocekavane odezve [$]d[/$]:
                             <br>
                             [$$]w_{cj} \\leftarrow (1-\\beta)w_{cj} + \\gamma z_c d_j[/$$]
                             <br>
                             Kde [$]\\beta>0, \\gamma>0, z_c[/$] je aktivita
                             viteze z Kohonenovske vrstvy
                        </ol>
        -       guid: ANg[8pi(z*
                topic: RBF sítě
                front: Definice RBF funkce a co ta zkratka znamená
                back: |
                        Radial Basis Functions.<br>
                        [$$]RBF(x,w,\\sigma) := \\exp\\{-\\frac{(x-w)^2}{2\\sigma^2}\\}[/$$]
        -       guid: e1;G#[k])1
                topic: RBF sítě
                front: Jakou mají architekturu?
                back: |
                        Vstup &rarr; Kohonenovská vrstva s Gaussovskou
                        přenosovou funkcí &rarr; lineární asociátor &rarr;
                        skalární výstup
        -       guid: EGy`up1IDl
                topic: RBF sítě
                front: Jak počítá?
                back: |
                        Neuron [$]j[/$] počítá výstup
                        [$]g_j(x)=RBF(x, w_j, \\sigma_j) / \\sum_k RBF(x, w_k, \\sigma_k)[/$].<br>

                        [$]\\sigma_i[/$] jsou konstanty: například nastavené podle vzdálenosti mezi váhovým vektorem a jeho nejbližším sousedem<br>

                        Z RBF vrstvy vedou váhy [$]z_1\\ldots z_m[/$].<br>
                        Ty jdou nastavit třeba zpětným šířením.
        -       guid: o_xi!X!vl4
                topic: RBF sítě
                front: |
                        Jak nastavit váhy [$]z_i[/$] z Kohonenovských neuronů
                        k lineárnímu asociátoru?
                back: |
                        [$]
                        E=\\frac{1}{2}\\sum_p (\\sum_{i=1}^n g_i(x_p)z_i - d_p)^2
                        [/$]<br>

                        [$]
                        \\Delta z_i \\approx -\\partial E/\\partial z_i =
                        \\gamma g_i(x) (d - \\sum_{j=1}^n g_j(x) z_j)
                        [/$]
        -       guid: vG)m(6Lxap
                topic: ART
                front: Architektura, jaké je kódování
                back: |
                        2 vrstvy: vstupní, výstupní.<br>

                        Vstupy: binární. Učí se bez učitele.<br>

                        Spojení: vstupy &rarr; výstupy, výstupy &rarr; vstupy,
                        výstupy mezi sebou (ale bez smyček z neuronu zpět do
                        něj)<br>

                        Má mechanismus pro vypnutí výstupního neuronu
                        s maximální odezvou.
        -       guid: sH:*w{Kntx
                topic: ART
                front: Účel laterálních spojů mezi výstupními neurony
                back: |
                        Určení výstupního neuronu s maximální odezvou.
        -       guid: O3zl2LcJ]!
                topic: ART
                front: Účel zpětných vazbeb z výstupů ke vstupům
                back: |
                        Porovnání skutečné podobnosti s rozpoznaným vzorem.
        -       guid: iHRTo?w_)%
                topic: ART
                front: Problém
                back: |
                        I při troše šumu začne ukládat moc vzorů
        -       guid: Q6&X_G@[*y
                topic: ART
                front: Značení - váhy ze vstupu [$]i[/$] do výstupu [$]j[/$]
                back: |
                        [$]b_{ij}[/$]
                include_reverse: true
        -       guid: sG*@FzF<Ve
                topic: ART &mdash; Značení
                front: Váhy z výstupu [$]j[/$] do vstupu [$]i[/$]
                back: |
                        [$]t_{ij}[/$]
                include_reverse: true
        -       guid: FKv,&dt)0O
                topic: ART &mdash; Značení
                front: Výstupy výstupních neuronů
                back: |
                        [$]\\mu_j[/$]
                include_reverse: true
        -       guid: I2=2o.GvXd
                topic: ART
                front: Značení vah a parametrů, jejich inicializace
                back: |
                        <ul>
                        <li>
                        [$]t_{ij}(0) = 1[/$] (váhy z výstupu do vstupu &mdash;
                        vzor specifikovaný výstupním neuronem [$]j[/$])
                        <li>
                        [$]b_{ij}(0)=1 / (1+N)[/$] (váhy ze vstupu do výstupu)
                        <li>
                        [$]\\rho\\in[0;1][/$] &mdash; jak blízko musí být vstup
                        k uloženému vzoru, aby byl ve stejné kategorii.
                        <li>
                        Výstupy výstupních neuronů: [$]\\mu_j[/$]
                        </ul>
        -       guid: g-#?rD)ZYT
                topic: ART &mdash; Značení
                front: Konstanta bdělosti
                back: |
                        [$$]\\rho\\in[0;1][/$$]
                include_reverse: true
        -       guid: cU$P=wEFXO
                topic: ART
                front: Učící algoritmus
                back: |
                        <ul>
                        <li> Předlož nový vstup [$]x[/$].
                        <li> Spočítej výstupy:
                             [$]\\mu_j = \\sum_{i=1}^{N} b_{ij}(t) x_i[/$]
                        <li> Vyber nejbližší odpovídající vzor:
                             [$]j^{ * } \\leftarrow \\arg\\max_j\\{\\mu_j\\}[/$]
                        <li> Test bdělosti:
                             [$]\\|x\\|=\\sum_i x_i,
                                \\|T\\cdot x\\|=\\sum_i t_{ij} \\cdot x_i[/$]
                             <br>
                             Když [$]\\frac{\\|T\\cdot x\\|}{\\|x\\|}<\\rho[/$],
                             dočasně nastav [$]\\mu_{j^{ * }}\\leftarrow 0[/$],
                             znova vyber nejlepší neuron bez účasti
                             [$]j^{ * }[/$].
                        <li> Aktualizace nejlépe odpovídajícího neuronu:
                             [$]t_{ij^{ * }}(t+1) = t_{ij^{ * }}(t) \\cdot x_i[/$],
                             [$]b_{ij^{ * }}(t+1) =
                             \\frac{t_{ij^{ * }} \\cdot x_i}
                                   {0.5 + \\sum_{k=0}^{N-1} t_{kj^{ * }}(t) \\cdot x_k}
                             [/$]
                        <li> Odmraž a opakuj.
                        </ul>
        -       guid: zJ#{leEhwF
                front: Kaskádová korelace, algoritmus učení
                markdown: true
                back: |
                        Začnu s přímým propojením vstupů na výstup.<br>
                        Přidávám skryté neurony.<br>
                        Vstupy nového jsou propojeny se všemi
                        předchozími vstupy a výstupy dřívějších neuronů.
                        <ol type="1">
                        <li> Adaptuj existující síť QuickPropem.
                             Když je malá chyba, konči.
                        <li> Přidej neuron, co maximalizuje korelaci svého
                             výstupu s chybou sítě. V další iteraci zmraž váhy
                             jeho vstupů, douč váhu jeho výstupů.
                        </ul>
                        Maximalizuj:
                        [$]S\equiv |\sum_{i=1}^p (V_i-\overline{V})(E_i-\overline{E})|[/$]
                        <br>
                        [$]V_i[/$]: výstup nového neuronu na vzoru [$]i[/$],
                        [$]\overline{V}[/$]: průměrný výstup nového
                        neuronu, to samé chyba
                        <br>
                        [$]\partial S/\partial w_k = \sum_p \delta_p I_{pi}[/$],
                        kde:
                        <br>
                        [$]\delta_p = \sum_o \sigma_o(e_{po}-\overline{e_j})f'_p[/$]
                        <br>
                        [$]\sigma_o[/$]: znaménko korelace mezi novým
                        neuronem a reziduální chybou na výstupu [$]o[/$]<br>
                        [$]f'_i[/$]: derivace přenosové funkce ve výstupní
                        jednotce<br>
                        [$]I_{pi}[/$]: [$]i[/$]-tý vstup nového neuronu pro
                        vzor [$]p[/$]
