deck: All::Magisterské státnice::Neuronové sítě::Přednáška 8 (učení bez učitele)
notes:
        # 08-unsupervised
        -       guid: ',m^d)rx:J'
                front: Jaký vztah mezi neurony se používá k učení bez učitele?
                back: |
                        Inhibice: neurony soutěží o rozpoznání vzoru. "Winner takes all."
        -       guid: PJGAQ:(<=~
                front: Síť na kompetiční učení bez učitele
                back: |
                        Neurony počítají Euklidovskou vzdálenost.
                        Vyhraje nejbližší, a přes laterální spoje potlačí ostatní.
                        Vítěz se posune o [$]\\alpha[/$] směrem k vstupu.
        -       guid: k.|x[s4[V8
                front: |
                        Síť na kompetiční učení bez učitele &mdash;
                        vhodná inicializace vah
                back: |
                        Váhy budou náhodné vzory z datasetu.
        -       guid: Jpug&5}h(j
                front: Problémy v síti na kompetiční učení bez učitele
                back: |
                        Mrtvé neurony: v Kohonenově mapě pomáhá mřížka
        -       guid: O$x}Zg`b^A
                markdown: false
                front: |
                        Energetická funkce sítě na kompetiční učení bez učitele
                        s jedním neuronem. Kde je optimum? Jak se to dokáže?
                back: |
                        [$$]E_X(w)=\sum_{i=1}^m \|x_i-w\|^2[/$$]<br>

                        Optimum je v těžišti:<br>

                        [latex]
                        \begin{align*}
                        E & = & \sum_{i=1}^m \|x_i-w\|^2 = \sum_i\sum_j(x_{ij}-w_j)^2= \\
                          & = & \sum_i \sum_j (x_{ij}^2 - 2x_{ij}w_j + w_j^2) = \\
                          & = & m\left( (\sum_j w_j^2) - \frac{2}{m}\sum_j w_j (\sum_i x_{ij}) \right) + \sum_i \sum_j x_{ij}^2 = \\
                          & = & m\sum_j (w_j^2 - \frac{2}{m}\sum_j(\sum_i x_{ij}) +
                                \frac{1}{m^2}(\sum_i x_{ij})(\sum_i x_{ij}))
                                \underbrace{- \frac{1}{m}\sum_{j=1}^n (\sum_i x_{ij})(\sum_i x_{ij}) + \sum_i \sum_j x_{ij}^2}_{=K} = \\
                          & = & m(\sum_j (w_j - \frac{1}{m}\sum_i x_{ij})^2) + K = \\
                          & = & m\|w-x^{ * }\|^2 + K
                        \end{align*}
                        [/latex]

        -       guid: ItH89MV4lO
                front: |
                        [$]k[/$]-means algoritmus
                back: |
                        Update: nastav středy jako těžiště jejich shluků;
                        Přenastav příslušnosti ke shlukům
        -       guid: s8>,r)$r,4
                front: Jak pro kompetiční učení bez učitele předzpracovat vstup?
                back: Vstupní vektory se znormují
        -       guid: Agc7GyhfKq
                front: |
                        Různá pravidla učení pro sítě na kompetiční učení
                        bez učitele
                back: |
                        <ul>
                        <li> Aktualizace pomocí parametru učení:
                             [$]\\Delta w = \\eta x_j[/$]
                        <li> Diferenční:
                             [$]\\Delta w = \\eta (x_j - w)[/$]<br>
                        <li> Dávková aktualizace
                        </ul>
        -       guid: M}~N<0CzeR
                topic: Stabilní rovnovážný stav v kompetičním učení
                front: |
                        Definice: Svazek
                back: |
                        [$]\\{x = \\alpha_1 p_1 + \\ldots \\alpha_m p_m : \\alpha_1 \\in \\R^+\\}[/$]
        -       guid: o[qOQC9+mX
                topic: Stabilní rovnovážný stav v kompetičním učení
                front: |
                        Definice: Průměr svazku
                back: |
                        [$]\\varphi(S) = \\sup\\{\\arccos (a\\cdot b) :
                        \\|a\\|=\\|b\\|=1, a,b\\in S\\}[/$]
        -       guid: s{R!t--...
                front: Stabilní rovnovážný stav v kompetičním učení - jaká je postačující
                        podmínka?
                back: |
                        Chceme jasně ohraničené shluky, co se nepřekrývají a
                        nejsou moc rozsáhlé.<br>
                        Postačující podmínka pro stabilitu: úhlový průměr
                        svazků je menší než vzájemná vzdálenost (jakmile se
                        střed shluku dostane do svazku, už tam zůstane)
        -       guid: l~f.YL-zf{
                topic: PCA
                front: Zkratka
                back: Principal component analysis
        -       guid: s=PlT/OZ-O
                topic: PCA
                front: Co to je? Co dělá?
                back: |
                        Hlavní komponenta je ta, co vysvětluje (ve vystředěných
                        a normalizovaných datech)
                        nejvíce rozptylu.<br>

                        Ostatní jsou na ni ortogonální.
        -       guid: tG$/)IO0IM
                topic: PCA
                front: Obecný algoritmus na výpočet
                back: |
                        Hlavní komponenta: maximalizuje [$]\\frac{1}{m} \\sum_{i=1}^m (w \\cdot x_i)^2[/$]<br>
                        Druhá: odečteme od vektorů jejich projekci na první a pokračujeme
        -       guid: qb1YQ8*d1=
                topic: Ojův algoritmus
                front: Cíl
                back: Hledá hlavní komponentu v matici pro PCA
        -       guid: pVt3i@Japq
                topic: Ojův algoritmus
                front: Co předpokládá?
                back: Těžiště dat je v počátku
        -       guid: Le41JE{*{6
                topic: Ojův algoritmus
                front: Jak probíhá?
                back: |
                        Náhodně inicializuje vektor [$]w[/$], zvolí parametr
                        učení [$]\\gamma\\in(0;1][/$]<br>

                        Náhodně vyber [$]x[/$], spočítej [$]\\Phi=x\\cdot w[/$],
                        nový váhový vektor je [$]w+\\gamma\\Phi(x-\\Phi w)[/$].
                        Zmenši [$]\\gamma[/$] a opakuj.<br>

                        Váhový vektor se "automaticky normalizuje".
        -       guid: w%Qg~g.oZI
                topic: Ojův algoritmus
                front: Konvergence a důkaz
                back: |
                        Idea: začne-li Ojův algoritmus ve svazku, bude v něm oscilovat, ale neopustí ho.<br>
                        Pro [$]\\|w\\|=1[/$] je [$]\\Phi=x\\cdot w[/$] délka projekce [$]x[/$] na [$]w[/$]<br>

                        Vektor [$]x-\\Phi w[/$] je kolmý na [$]w[/$].<br>

                        Iterace přitahují [$]w[/$] k vektorům ze shluku X.<br>

                        Pokud je délka [$]w[/$] blízko 1, umístí se [$]w[/$] v centru shluku.<br>

                        Dále to chce dokázat, že Ojův algoritmus automaticky normuje [$]w[/$].
        -       guid: K5Og;4w[zA
                front: Problémy Ojova algoritmu, zobecnění
                back: |
                        "Řidké" shluky, příliš velké rozdíly v délce vstupních vektorů.
                        Dá se zobecnit, aby najednou počítal více nejdůležitějších příznaků.
