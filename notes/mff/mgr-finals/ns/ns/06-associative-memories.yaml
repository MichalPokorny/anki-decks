deck: All::Magisterské státnice::Neuronové sítě::Přednáška 6 (asociativní paměti)
notes:
        # 06-associative-memories
        -       guid: n49K;VjOO!
                front: Cíl asociativních sítí versus BP sítí
                back: |
                        Zobrazit zašuměné okolí vzoru přesně na správný výstup.
                        BP sítě: okolí vstupu se zobrazuje na okolí správného výstupu.
        -       guid: ppoT~!QaGx
                front: 3 typy asociativních sítí
                back: |
                        <ul>
                        <li> <i>Heteroasociativní</i>:
                        když jsem do [$]\\varepsilon[/$] vzdálenosti od
                        správného vzoru, namapuje se správně.
                        <li> <i>Autoasociativní</i>: "oprava zašuměných vzorů"
                        <li> <i>Pattern recognition síť</i>: přiřadí skalár
                        </ul>
        -       guid: wNMjAD=yJ;
                front: Jednovrstevnatá heteroasociativní síť bez zpětné vazby
                back: |
                        [$]y = x \cdot W[/$], kde [$]y[/$] ma [$]k[/$] slozek, [$]x[/$] jich ma [$]n[/$].<br>
                        Hledáme matici [$]W[/$] takovou, aby [$]X \cdot W = Y[/$].
        -       guid: l8/#Q|r{28
                front: |
                        Jednovrstevnatá heteroasociativní síť bez zpětné vazby
                        - co když je [$]m=n[/$]?
                back: |
                        [$]X[/$] je čtvercová. Má-li inverzi,
                        tak [$]W=X^{-1}\cdot Y[/$].
        -       guid: ~87f[fEL=
                front: |
                        Rekurentní autoasociativní síť bez step function -
                        vlastní vektory
                back: |
                        Existuje-li pevný bod [$]\\xi \cdot W = \\xi[/$],
                        tak to je vlastní vektor s [$]\\lambda=1[/$].
        -       guid: k!L5{|XsX8
                front: |
                        Co se stane, když matici s různými [$]\\lambda[/$] budu
                        neustále násobit jedním vektorem?
                back: |
                        Dostanu z toho největší vlastní číslo.
                        Jeho eigenvektor je atraktor.
        -       guid: b(`[)^-nuK
                front: Hebbovské učení
                back: |
                        Síť má přenosovou funkci [$]\\sgn[/$], je jednovrstvá.
                        Vstupní neurony mají na začátku [$]x\\in\\R^n[/$],
                        výstupní mají [$]y\\in\\R^k[/$].

                        Adaptační pravidlo:
                        [$]\\Delta w_{ij} = \\gamma x_i y_j[/$]

                        Váhová matice:
                        [$]W^{(1)}_{ij} = x^{(1)}_i \\cdot y^{(1)}_j[/$]
        -       guid: L@hjZ:-p;h
                topic: Hebbovské učení
                front: |
                        Proč používá bipolární kódování, ne binární?
                back: Bipolární vektory mají větší šanci na ortogonalitu.
        -       guid: BY(O=qCp}y
                topic: Hebbovské učení
                front: |
                        Výstup Hebbovské sítě natrénované na matici pro
                        [$] (x,y)[/$] na nenulovém vektoru [$]x[/$], [$]-x[/$]?
                back: |
                        [$]y[/$], [$]-y[/$]
        -       guid: g0|SD{)qJm
                topic: Hebbovské učení
                front: |
                        Matice sítě pro Hebbovské učení na trénovacích
                        vzorech [$] (x_1,y_1), (x_2,y_2), \\ldots[/$]
                back: |
                        [$]W=W^{(1)} + W^{(2)} + \\ldots + W^{(m)}[/$]

                        [$]W^{(1)}_{ij}=x^{(1)}_i y^{(1)}_j[/$]
        -       guid: pPRh:f9Rw/
                topic: Hebbovské učení
                front: Rozdělení výstupu na správný výstup a crosstalk?
                markdown: false
                back: |
                        [latex]
                        \begin{align*}
                        x^{(p)} \cdot W & = & x^{(p)} \cdot (W^{(1)} + \ldots) = \\
                                        & = & x^{(p)} W^{(p)} + \sum_{l\neq p} x^{(p)} \cdot W^{(l)} = \\
                                        & = & y^{(p)} \cdot (x^{(p)} \cdot x^{(p)}) +
                                              \begin{underbrace}{\sum_{l\neq p} y^{(l)}\cdot (x^{(l)} \cdot x^{(p)})}_{\text{crosstalk}}
                        [/latex]
        -       guid: r&<<OpB/-!
                topic: Hebbovské učení
                front: Jak by měl vypadat crosstalk pro dobré výsledky?
                back: |
                        Výstup:
                        [$$]
                        \\sgn(x^{(p)}\\cdot W)=\\sgn(y^{(p)} \\cdot (x^{(p)} \\cdot x^{(p)}) + \\sum_{l\\neq p}y^{(l)} \\cdot (x^{(l)} \\cdot x^{(p)}))
                        [/$$]<br>

                        Protože [$]x^{(p)} \\cdot x^{(p)} > 0[/$]:
                        [$$]
                        \\sgn(x^{(p)} \\cdot W)=\\sgn(y^{(p)} + \\sum_{l\\neq p}y^{(l)} \\cdot \\frac{x^{(l)} \\cdot x^{(p)}}{x^{(p)} \\cdot x^{(p)}})
                        [/$$]

                        Chci, aby absolutní hodnota všech složek
                        [$]\\sum_{l\\neq p}y^{(l)} \\cdot \\frac{x^{(l)} \\cdot x^{(p)}}{x^{(p)} \\cdot x^{(p)}}[/$] byla menší než 1.
        -       guid: LX0E/IjP}2
                front: |
                        Rekurentní asociativní paměť vs. bez zpětné vazby
                back: |
                        Sféry vlivu atraktorů jsou v rekurentní větší.
                        Ale zase rekurentní asociativní paměť má menší kapacitu.
        -       guid: F<%IIby-Rb
                front: |
                        Maximální kapacita autoasociativní Hebbovské sítě podle
                        dimenze vstupního vektoru
                back: |
                        [$]m\\sim 0.18n[/$], kde [$]n[/$] je dimenze vstupu, [$]m[/$] je počet uložených vzorů.
                        Předpokládáme nekorelované vzory.
        -       guid: xW3^0Msq6x
                front: |
                        Maximální kapacita autoasociativní Hebbovské sítě podle
                        dimenze vstupního vektoru &mdash; je to [$]m=0.18n[/$],
                        jak se to odvodí?
                back: |
                        Chceme pro každý bit [$]i[/$] mít tohle menší než 1:
                        [$]
                        \\frac{1}{n} \\sum_{l\\neq p} x_i^{(l)}(x^{(l)} \\cdot x^{(p)})
                        [/$]

                        Tedy dostaneme [$]m\\cdot n[/$] náhodných hodnot, očekávaná hodnota je 0 (bipolární).

                        Součet má binomické rozdělení, pro aproximujeme ho
                        normálním rozdělením s [$]\\sigma = \\sqrt{m/n}[/$].
                        Mrkneme se do tabulek, jak velký je interval že součet
                        vyjde uvnitř [$]\\pm 1[/$]. Strčíme do toho, že chceme
                        pravděpodobnost chyby [$]0.01[/$].
                        Dostaneme [$]m\\sim 0.18n[/$].
        -       guid: ywud0]p5Yn
                front: |
                        Proč můžeme chtít použít pseudoinverzní matici místo
                        Hebbovského učení?
                back: |
                        Protože vzory můžou být korelované, takže nevhodné pro
                        přímé Hebbovské učení s korelační maticí.
        -       guid: B}M/s|$Zwl
                front: K čemu je pseudoinverzní matice?
                back: |
                        Hledáme váhovou matici pro asociační paměť že [$]XW=Y[/$].
                        Ale k [$]X[/$] nemusí existovat inverze, tak hledáme
                        matici co minimalizuje [$]\\|XW-Y\\|^2[/$], to
                        minimalizuje [$]W=\\tilde{X}Y[/$]
        -       guid: d=:E82jWtJ
                front: |
                        Použití pseudoinverzní matice pro minimalizaci odchylky
                        v single-layer BP síti
                back: |
                        Odchylka:
                        [$]E=\\sum_{p=1}^P\\sum_{j=1}^m (d_{j,p}-y_{j,p})^2[/$]
                        <br>
                        [$]y_{j,p}=\\sum_{i=1}^n w_{ij}x_{i,p}[/$]
                        <br>
                        Gradient:
                        [$$]\\partial E/\\partial w_{ij} =
                        -2\\sum_{p=1}^P (\\sum_{i=1}^n d_{j,p}-w_{ij} x_{i,p})x_{i,p}[/$$]
                        Budeme ho chtít mít nulový.
                        <br>
                        Maticově: [$]WXX^T=DX^T[/$]. K [$]XX^T[/$] obecně nemusí
                        existovat inverzní matice.<br>

                        Řešení může být víc: omezíme velikosti vah:
                        [$]E += \\lambda\\sum_{i=1}^n \\sum_{j=1}^m w_{ij}^2[/$].<br>

                        Minimalizace přes parciální derivace:
                        [$]W(XX^T+\\lambda I)=DX^T[/$]<br>

                        Pro [$]\\lambda>0[/$] existuje inverze k [$]XX^T+\\lambda I[/$],
                        tak to takhle vyřešíme pro [$]W[/$]:<br>
                        [$$]
                        W(XX^T+\\lambda I)(XX^T+\\lambda I)^{-1}=DX^T(XX^T+\\lambda I)^{-1}
                        [/$$]<br>

                        Limitně pro [$]\\lambda\\rightarrow 0[/$]: [$]W = D\\tilde{X}[/$].
                        Když je více řešení, tak tohle minimalizuje součet čtverců vah.
        -       guid: jn*LEs=JXy
                front: |
                        Výpočet pseudoinverzní matice přes vrstevnaté neuronové
                        sítě. K čemu se hodí?
                back: |
                        Zkonstruuju síť, pro kterou střední kvadratická
                        odchylka odpovídá [$]\|XW-Y\|^2[/$], backprop se bude
                        snažit najít [$]W[/$] které to minimalizuje, tedy
                        pseudoinverzi.
                        <br>
                        Hodí se k nalezení vah pro asociativní síť (BAM).
