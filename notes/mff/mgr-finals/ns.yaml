deck: Magisterské státnice::Neuronové sítě
notes:
        -
                uuid: fd1046a3-c791-47a7-bc0b-61af423e6ab1
                front: Sigmoida s parametrem
                back: >
                        [$$]\\sigma(t)=(1+\\exp(-\\lambda t))^{-1}[/$$]
        # 01-introduction
        -
                uuid: 9750b835-3a32-4395-9cca-cd9e09bf5fe6
                front: Části neuronu jako buňky
                back: tělo (soma), dendrity, axonová vlákna, synapse
        -
                uuid: 668d4ed1-632f-48df-a543-0198a6732319
                front: Velikost somy neuronu
                back: Několik jednotek až desítek mikrometrů
        -
                uuid: a3030c28-d62f-4c8a-b279-36e89f787626
                front: Délka dendritů
                back: Kolem 2-3 mm
        -
                uuid: 7a033fae-6632-4497-8d99-d2e6a9b8a282
                front: Axony, délka, větvení
                back: Jediné výstupy neuronu, bohatě větvené na konci. Délka až 1 m.
        -
                uuid: 5168249c-6b4e-47fb-80f8-9eb5300f2f7b
                front: Počet synapsí na neuron
                back: Až [$]10^6[/$]
        -
                uuid: 457b2357-c57f-4629-ac19-5d94d0c1dea9
                front: Hustota neuronů v lidském mozku
                back: >
                        [$]7-8\\cdot 10^4/mm^3[/$]
        -
                uuid: 789e094f-ac4e-45a2-b541-319663b98dba
                front: Rychlost odumírání neuronů
                back: >
                        [$]\\sim 10^4[/$] denně. Synapse se ale vytvářejí celý život.
        -
                uuid: d8623246-ee29-491e-8de4-ac15ebb3f88e
                front: Kolik vrstev má povrch neuronu?
                back: Dvě; jsou z lipidů
        -
                uuid: d5292ecb-f9f6-456f-84f6-054ac7bbef39
                front: Co je mezi vrstvami povrchu (membrány) neuronu?
                back: Vnitromembránové proteiny, co tvoří iontové pumpy a kanály
        -
                uuid: 0f0c90dc-5bf1-489d-836e-af7c5858e50a
                front: Co je funkce iontových kanálů?
                back: Řídí propustnost membrány s ohledem na typ iontů -- jsou oddělené kanály na draslíkové, sodíkové ionty
        -
                uuid: bb2ea72e-dedc-4fba-95ca-261cad4205f3
                front: Co je funkce iontových pump?
                back: Trvale přenášejí ionty draslíku a sodíku, tedy polarizují membránu.
        -
                uuid: 7b90070a-c895-4e86-9c31-8e7660c26769
                front: Polarizace neuronového povrchu - jaká je vevnitř, venku?
                back: Venku kladná, vevnitř záporná
        -
                uuid: 4fa5ac9c-ef80-4c9c-b683-7ac6f4cdb537
                front: Rozdíl potenciálů po stranách neuronové membrány
                back: Kolem -70 mV
        -
                uuid: 9e37fe36-e0fd-4464-8967-a9c10809dc60
                front: Dva typy neuronových membrán
                back: >
                        Vodivá (myelinový povlak), transmisní<br>

                        Vodivá: má v sobě Ranvierovy zářezy (zrychlují přenos asi 50x a snižují zkreslení)<br>
                        Transmisní: navíc má v sobě receptorové proteiny, co zavírají/otevírají iontové kanály
        -
                uuid: 10df7441-510e-405b-b79a-ce9eff9c001b
                front: Krátkodobý paměťový mechanismus
                back: >
                        Založený na cyklickém oběhu vzruchů v NS.
                        Když proběhne asi 300x, zafixuje se ve střednědobé paměti. To trvá asi 30 s.
        -
                uuid: 208fccf6-d030-4dcd-b260-79a125792929
                front: Střednědobý paměťový mechanismus
                back: Založený na "změnách vah". Uchovává se několik hodin až dnů. Ve spánku se přenáší do dlouhodobé paměti.
        -
                uuid: a42eb832-b4a5-40ab-b046-6952f70d0d43
                front: Dlouhodobý paměťový mechanismus
                back: Kopíruje střednědobou paměť do bílkovin v neuronech, hlavně v jádrech. Někdy zůstanou až celý život.
        # 02-perceptron: nothing
        # 03-multilayered-nns
        -
                uuid: 5b2844e9-94ba-40a5-af9f-47a7153c9440
                front: Chybová funkce pro feedforward sítě
                back: >
                        [$]E=\\frac{1}{2}\\sum_{p} \\sum_{j} (y_{j,p}-d_{j,p})^2[/$],
                        kde [$]j[/$] je index ve výstupním vektoru, [$]p[/$] je vzor
        -
                uuid: b922cbf1-635a-456b-90b2-14e5f04a6352
                front: Jak vypadá update pro feedforward sítě, obecně?
                back: >
                        [$$]
                        w_{ij} -= \\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial y_j}\\frac{\\partial y_j}{\\partial \\xi_j}\\frac{\\partial \\xi_j}{\\partial w_{ij}}
                        [/$$]
        -
                uuid: 78dc62d7-6f8f-4ebc-9bd3-4efafc2ccbc3
                front: Co je v updatu pro feedforward sítě [$]\\delta[/$]?
                back: >
                        [$$]
                        \\delta_j := -\\frac{\\partial E}{\\partial \\xi_j}
                        [/$$]
        -
                uuid: cbaf91a5-83a2-43ee-a16d-61a019357318
                front: Jak vypadá role [$]\\delta[/$] v updatovacím pravidle?
                back: >
                        [$$]
                        w_{ij} += \\delta_j \\frac{\\partial \\xi_j}{\\partial w_{ij}} = \\delta_j y_i
                        [/$$]
        -
                uuid: d4a36360-45fe-44e5-ac35-c417012061a9
                front: Plná forma updatu pro váhy do výstupní vrstvy
                back: >
                        [$$]
                        w_{ij} -= (y_j - d_j) f'(\\xi_j) y_i
                        [/$$]
        -
                uuid: 111c73aa-eade-4989-bc69-2f4cec56779b
                front: Plná forma updatu pro váhy do skrytých vrstev
                back: >
                        [$$]
                        w_{ij} -= (\\sum_k \\frac{\\partial E}{\\partial \\xi_k} w_{jk}) f'(\\xi_j) y_i =

                        = (\\sum_k \\delta_k w_{jk}) f'(\\xi_j) y_i
                        [/$$]
        -
                uuid: e40d07d1-565a-40f1-8066-0c313967d318
                front: Derivace sigmoidy s parametrem [$]\\lambda[/$] vzhledem k její hodnotě
                back: >
                        [$]f'(x) = \\lambda f(x) (1-f(x))[/$]
        -
                uuid: 3e6d5447-bf83-456a-aba4-d6bc537ab880
                front: Aktualizace vah s momentem
                back: >
                        [$$]
                        w_{ij}^{(t+1)} = w_{ij}^{(t)} + \\alpha \\delta_j y_i +
                                \\alpha_m (w_{ij}^{(t)} - w_{ij}^{(t-1)})
                        [/$$]
        -
                uuid: 855893e2-9a41-4b34-9a45-c05098fce827
                front: Velikost momentu učení
                back: >
                        [$]\\alpha_m \\in [0;1)[/$]
        -
                uuid: d07c9791-5d35-47e5-a396-4e0c984c81a9
                front: Složitost učení neuronových sítí
                back: NP-úplný
        -
                uuid: c545814c-390b-4ff6-86c0-d2bd755436ef
                front: Chytrá volba počátečních vah - proč nulově vycentrované?
                back: Budou v intervalu [$][-\\alpha_m;\\alpha_m][/$]. Protože přenosová funkce má v 0 maximální derivaci. Šířená chyba je pak větší.
        -
                uuid: 2cd4abd6-9386-4cd9-8abb-4049fdfd8ece
                front: Proč jsou problém příliš velké i příliš malé váhy?
                back: >
                        Příliš malé: Šířená chyba je pak moc malá.
                        Příliš velké: chybová funkce se dostane do saturované zóny, je plochá.
        -
                uuid: f3485758-c4a8-4f09-b934-0adceccebb6e
                front: Chytrá volba vah - jak nanormalizujeme vstupy?
                back: >
                        Normalizujeme je do [$][0;1][/$].
        -
                uuid: e7ea5e2f-d109-457c-aadd-4a09155b21ac
                front: Jaká bude střední hodnota potenciálu když nainicializujeme váhy náhodně v [$]\\alpha_m[/$] kolem nuly?
                back: >
                        [$$]
                        E[\\xi_j] = E[\\sum_{i=0}^n w_{ij} x_i]=\\sum_{i=0}^n E[w_{ij}] E[x_i] = 0
                        [/$$]
        -
                uuid: 6bfe2886-12e0-4001-9652-80e9da1b051b
                front: Chytrá volba vah - odvození rozptylu potenciálu
                back: >
                        [$$]
                        \\sigma_{\\xi_j}^2 = E[ (\\xi_j)^2] - (E[\\xi_j])^2 = E[ (\\sum_{i=0}^n w_{ij} x_i)^2] - 0 =
                        [/$$]
                        [$$]
                        = \\sum_{i,k=0}^{n} E[w_{ij} w_{kj} x_i x_k] = \\sum_{i=0}^n E[(w_{ij})^2] E[(x_i)^2]
                        [/$$]
        -
                uuid: 27dde7d4-ec20-4d03-a645-c191998c0b62
                front: Chytrá volba vah - jaká je střední hodnota [$] (x_i)^2[/$] když je [$]x[/$] rovnoměrně v [$][0;1][/$]?
                back: >
                        [$]1/3[/$]
        -
                uuid: 01f20ae2-2fc3-4490-aec4-d1da73e92863
                front: Chytrá volba vah - jaká je střední hodnota [$] (w_{ij})^2[/$] když je [$]w_{ij}[/$] rovnoměrně v [$][-a;a][/$]?
                back: >
                        [$]a^2/3[/$]
        -
                uuid: 3e982b3e-298b-4aab-9eb9-a3f564d0cfcd
                front: Když chci aby měl potenciál směrodatnou odchylku [$]A=\\sigma_{\\xi}[/$], jak mám zvolit interval [$][-a;a][/$] pro vstupní váhy? Jaká je chytrá volba A?
                back: >
                        [$]a = 3A/\\sqrt{N}[/$] ([$]A=\\sigma_\\xi = a/3 \\sqrt{N}[/$]). Chytrá volba: A=1 (rychlé učení, velký gradient)
        -
                uuid: 544ed41c-952b-40b9-aad3-9118a9ddce3f
                front: K čemu je moment?
                back: Snižuje oscilaci, tedy urychluje učení
        -
                uuid: 3bd20133-9998-45b8-84e0-7f980f6435b4
                front: Adaptivní parametr učení -- obecně
                back: Pro každou váhu samostatný parametr [$]\\alpha_{ij}[/$]
        -
                uuid: 486172e7-653f-443b-81bf-3aef15ba9cff
                front: Silva-Almeida heuristika
                back: Jestli se nezměnilo znaménko parciální derivace, urychluj ([$]\\cdot \\uparrow[/$]). Když se změnilo, zpomaluj ([$]\\cdot\\downarrow[/$]).
        -
                uuid: 7b3c1733-32ae-4cc9-b36b-b0c5ee1c9782
                front: Delta-bar-delta algoritmus, proč je lepší než Silva-Almeida
                back: >
                        Není tak citlivý na exponenciální růst když se moc dlouho zrychluje.

                        [$]\\delta_{i}^{(k)} = (1-\\Phi)\\frac{\\partial E}{\\partial \\xi_{i}} + \\Phi \\delta_i^{(k+1)}[/$],
                        kde [$]\\Phi[/$] je konstanta.

                        Když [$]\\delta_{i}^{(k-1)} \\cdot \\frac{\\partial E}{\\partial \\xi_{i}} > 0[/$], tak [$]\\alpha_{i} += \\uparrow[/$], jinak [$]/= \\downarrow[/$].

                        Aktualizace vah je bez momentu: [$]w_i -= \\alpha_i^{(k)} \\delta_i^{(k)} y_i^{(k)}[/$].
        -
                uuid: 337f8612-aa5f-4688-a6b3-2cf6b499bf65
                front: Super-SAB algoritmus
                back: >
                        Dělej backprop s momentem.
                        Začni s rychlostí 1.2.
                        Když se nezměnilo znaménko, vynásob rychlost 1.05.
                        Když se změnilo, undoni poslední změnu, pak poděl rychlost 2 a pro účely momentu v dalším updatu se tvař že rychlost v téhle souřadnici je 0.
        -
                uuid: 69dfb87b-0ab7-4953-a173-722046c8167f
                front: Taylorova řada pro aproximaci E druhého řádu
                back: >
                        [$]E(w+h) \\simeq E(w) + \\nabla E(w)^T \\cdot h + \\frac{1}{2}h^T \\nabla^2 E(w) h[/$]

                        Kde [$]\\nabla^2 E(w)[/$] je [$]n\\times n[/$] Hessovská matice, kde prvek [$]ij[/$] je [$]\\partial^2 E(w) / \\partial w_i \\partial w_j[/$].
        -
                uuid: 8606828b-9128-45ff-b000-7e22e0ad5b19
                front: Aproximace gradientu druhého řádu. O co se s ním snažíme?
                back: >
                        [$]\\nabla E(w+h)^T\\simeq\\nabla E(w)^T + h^T \\nabla^2 E(w)[/$]<br>

                        Snažíme se gradient [$]h = -(\\nabla^2 E(w))^{-1} \\nabla E(w)[/$] mít nulový, protože hledáme minimum [$]E[/$].
        -
                uuid: ce22d60a-439f-42ab-8126-003b0d1a71ea
                front: Newtonovské metody pro druhý řád a problémy
                back: >
                        Aktualizace vah:
                        [$$]w_{ij} -= (\\nabla^2 E(w))^{-1} \\nabla E(w)[/$$]
                        Problém: výpočet inverzní Hessovské matice
        -
                uuid: db9136b3-4c7d-439e-9089-a472eddbca40
                front: Pseudonewtonovské metody, proč se používají, jak zjednodušují newtonovské metody, kdy fungují
                back: >
                        Berou v úvahu jenom diagonálu Hessovské matice.
                        Adaptace: [$$]
                        w_{ij} -= \\frac{\\nabla_{ij} E(w)}{\\frac{\\partial^2 E(w)}{\\partial w_{ij}^2}}
                        [/$$]
                        Fungují dobře když chybová funkce má "kvadratický tvar".
        -
                uuid: a70f0d15-a2d4-46da-8214-3f342c831577
                front: Algoritmus QuickProp. Alternativní způsob psaní aktualizace. Čemu odpovídá jmenovatel?
                back: >
                        [$]w_{ij} += \\Delta^{(k)} w_i[/$]<br>

                        [$$]\\Delta^{(k)} w_i = \\Delta^{(k-1)} w_i \\cdot \\frac{\\nabla_i E^{(k)}}{\\nabla_i E^{(k-1)} - \\nabla_i E^{(k)}}[/$$]<br>

                        Jde psát taky:
                        [$$]
                        \\Delta^{(k)}w_i = -\\frac{\\nabla_i E^{(k)}}{\\frac{\\nabla_i E^{(k)}-\\nabla_i E^{(k-1)}}{\\Delta^{(k-1)}w_i}}
                        [/$$]<br>

                        Jmenovatel je diskrétní aproximace parciální derivace [$]\\partial^2 E(w) / \\partial w_i^2[/$]
        -
                uuid: f7eda12e-ea8c-4f7b-9241-e381ce03f077
                front: Levenberg-Marquardtův algoritmus
                back: >
                        Je rychlejší a přesnější v oblasti minima erf.
                        Kombinace gradientní a Newtonovy metody.
                        [$$]w_{\\min}=w_0-(H+\\lambda I)^{-1} \\cdot g[/$$], kde [$]g[/$] je gradient, [$]H[/$] je Hessovská matice<br>

                        Pro 1 výstup: [$]g_i = \\partial E/\\partial w_i = 2(y-d)\\frac{\\partial y}{\\partial w_i}[/$]<br>

                        [$$]\\partial^2 E/\\partial w_i \\partial w_j = 2[\\frac{\\partial y}{\\partial w_i}\\frac{\\partial y}{\\partial w_j} + (y-d)\\frac{\\partial^2 y}{\\partial w_i \\partial w_j}][/$$]<br>
                        Druhý člen -- ten s [$] (y-d)[/$] -- se zanedbá.<br>
        -
                uuid: 4e8953f3-7b3e-4d23-8c89-b618df5773a6
                front: Relaxační metody - perturbace vah
                back: >
                        Vždy pro náhodně zvolenou váhu si spočítám chybovou funkci když ji o trochu posunu, podle toho spočítám diskrétní aproximaci gradientu a aktualizuju.
        -
                uuid: ee40f752-837d-4d47-9787-f7bdbc09711b
                front: Alternativní perturbační metoda s rychlejší konvergencí
                back: >
                        Zperturbuju výstup neuronu [$]i[/$] ([$]o_i[/$]) o [$]\\Delta o_i[/$].
                        Spočítám rozdíl chyb. Jestli se chyba zlepší, tak si spočítám, jak má vypadat potenciál neuronu, aby měl tenhle výstup. Adaptuju váhy proporcionálně k jejich velikosti ([$]w_i'/\\xi' = w_i/\\xi[/$].
        # 04-multilayer-properties
        -
                uuid: ef9250b7-8401-4331-ab84-826be6c72ff8
                front: Realizovatelnost funkcí
                back: Každá funkce jde aproximovat prahovými jednotkami s libovolně malou chybou. To samé jde i se sigmoidami.
        -
                uuid: 57831b2e-2ac4-4bc6-b420-38e3dd343898
                front: Počet oblastí určených [$]m[/$] dělícími nadrovinami dimenze [$]n-1[/$] v [$]n[/$]-rozměrném prostoru -- rekurzivní vzorec
                back: >
                        [$]R(m,n)=R(m-1,n)+R(m-1,n-1)[/$]
        -
                uuid: 99ccc2b2-92aa-42f0-9324-395f522bd8ad
                front: Počet oblastí určených [$]m[/$] dělícími nadrovinami dimenze [$]n-1[/$] v [$]n[/$]-rozměrném prostoru -- closed-form vzorec
                back: >
                        [$]R(m,n)=2 \\sum_{i=0}^{n-1} { {m-1} \\choose {i}}[/$]
        -
                uuid: bab5c148-ed49-41aa-a7a9-e4adc2e8399a
                front: Počet vzorů potřebných pro správné zobecňování s generalizační chybou [$]\\varepsilon[/$], dle počtu vah [$]W[/$] a počtu neuronů [$]N[/$]. Rule of thumb?
                back: >
                        [$]P \\geq (W/\\varepsilon) \\log_2 (N/\\varepsilon)[/$]<br>

                        Pro 1 skrytou vrstvu: nemůže dobře zobecňovat když bylo méně než [$]W/\\varepsilon[/$] vzorů.<br>
                        Tedy: pro přesnost [$]90\\%[/$] chci aspoň [$]10\\cdot W[/$] vzorů.
        # 05-internal-representation
        -
                uuid: bc424437-cb79-4278-acce-3893f428a98e
                front: Kondenzovaná vnitřní reprezentace, binární; jak se učí?
                back: >
                        Interpretace skrytých neuronů: [$]0/1/\\frac{1}{2}[/$]<br>

                        Binární: jen 0/1<br>

                        Přidá se člen:
                        [$$]
                        F=\\sum_{p} \\sum_{h} y_{h,p}^X (1-y_{h,p})^Y (y_{h,p}-0.5)^2Z
                        [/$$]

                        U 1/2 to musí být sudé (symetrické).
        -
                uuid: adf31f57-65b6-40e5-9cae-d759e46052bf
                front: Jednoznačná vnitřní reprezentace; jak se naučí?
                back: >
                        Intuice: pro hodně různé výstupy chci hodně jiné vnitřní reprezentace<br>

                        [$$]
                        H = -\\frac{1}{2}\\sum_{p, q; p\\neq q} \\sum_{j} \\sum_{o} (d_{o,p}-d_{o,q})^2 (y_{j,p}-y_{j,q})^2
                        [/$$]
        -
                uuid: d14d2601-5db9-4c7e-a943-718b893ad9d4
                front: Prořezávání dle interní reprezentace
                back: Spoj neurony, co jsou pořád konstantní, nebo rovné jinému neuronu, nebo mu inverzní. Postupně zredukuj celou síť.
        # 06-associative-memories
        -
                uuid: 9f846394-8002-4e78-b804-13657d3b1969
                front: Cíl asociativních sítí versus BP sítí
                back: >
                        Zobrazit zašuměné okolí vzoru přesně na správný výstup.
                        BP sítě: okolí vstupu se zobrazuje na okolí správného výstupu.
        -
                uuid: ef5f7835-c422-4ff2-bb21-af8881f3c286
                front: Tři typy asociativních sítí
                back: >
                        Heteroasociativní: když jsem do [$]\\varepsilon[/$] vzdálenosti od správného vzoru, namapuje se správně.<br>
                        Autoasociativní: "oprava zašuměných vzorů"<br>
                        Pattern recognition síť: přiřadí skalární hodnotu
        -
                uuid: f88de9b7-ef95-4ead-ae75-cc1160ad4e89
                front: Jednovrstevnatá heteroasociativní síť bez zpětné vazby
                back: >
                        [$]y = x \cdot W[/$], kde [$]y[/$] ma [$]k[/$] slozek, [$]x[/$] jich ma [$]n[/$].<br>
                        Hledáme matici [$]W[/$] takovou, aby [$]X \cdot W = Y[/$].
        -
                uuid: c3a5e6b1-be02-4e1b-bff2-430ee6c2d1ef
                front: Jednovrstevnatá heteroasociativní síť bez zpětné vazby - co když je [$]m=n[/$]?
                back: >
                        Když je [$]m=n[/$], tak je [$]X[/$] čtvercová, a má-li inverzi, tak [$]W=X^{-1}\cdot Y[/$]
        -
                uuid: f762504c-ad36-4db0-b964-877035c03a31
                front: Rekurentní autoasociativní síť bez step function - vlastní vektory
                back: >
                        Existuje-li pevný bod [$]\\xi \cdot W = \\xi[/$], tak to je vlastní vektor s [$]\\lambda=1[/$].
        -
                uuid: 55c0f857-affd-4e65-8f0b-d18c047b00ac
                front: Co se stane, když matici s různými lambdami budu neustále násobit jedním vektorem?
                back: >
                        Dostanu z toho největší vlastní číslo. Jeho eigenvektor je atraktor.
        -
                uuid: f429ddf1-63a8-4764-9875-f6ebca02b5fc
                front: Hebbovské učení
                back: >
                        Síť má přenosovou funkci [$]\\sgn[/$], je jednovrstvá.
                        Vstupní neurony mají na začátku [$]x[/$], které je n-rozměrné, výstupní mají y, které je k-rozměrné.

                        Adaptační pravidlo:
                        [$$]\\Delta w_{ij} = \\gamma x_i y_j[/$$]

                        Váhová matice:
                        [$$]W^{(1)}_{ij} = x^{(1)}_i \\cdot y^{(1)}_j[/$$]
        -
                uuid: 104a8bf1-2f98-463f-bfe7-3e465da899e0
                front: Proč se v Hebbovském učení používá bipolární kódování, ne binární?
                back: Protože bipolární vektory mají větší šanci na ortogonalitu.
        -
                uuid: 62c57f98-ecf3-4202-89da-f2e7a7cbce49
                front: Výstup Hebbovské sítě natrénované na matici pro [$] (x,y)[/$] na nenulovém vektoru [$]x[/$], [$]-x[/$]?
                back: >
                        [$]y[/$], [$]-y[/$]
        -
                uuid: 42fd3ab6-5cbc-488b-b62d-1071d55681a3
                front: Jak vypadá matice sítě pro Hebbovské učení, která má trénovací vzory [$] (x_1,y_1), (x_2,y_2), \\ldots[/$]?
                back: >
                        [$]W=W^{(1)} + W^{(2)} + \\ldots + W^{(m)}[/$]

                        [$]W^{(1)}_{ij}=x^{(1)}_i y^{(1)}_j[/$]
        -
                uuid: d0f60e88-2117-4142-b014-77fc0fa4c53a
                front: Rozdělení výstupu Hebbovské sítě na správný výstup a crosstalk?
                back: >
                        [$$]
                        x^{(p)} \\cdot W = x^{(p)} \\cdot (W^{(1)} + \\ldots) = x^{(p)} W^{(p)} + \\sum_{l\\neq p} x^{(p)} \\cdot W^{(l)} =

                        = y^{(p)} \\cdot (x^{(p)} \\cdot x^{(p)}) + \\sum_{l\\neq p} y^{(l)} \\cdot (x^{(l)} \\cdot x^{(p)})
                        [/$$]

                        Crosstalk je poslední kus: [$]\\sum_{l\\neq p} y^{(l)} \\cdot (x^{(l)} \\cdot x^{(p)})[/$]
        -
                uuid: b4cb093c-66b9-4fad-b9a1-7ad8b27814a0
                front: Jak by měl vypadat crosstalk, aby síť dávala dobré výsledky?
                back: >
                        Výstup sítě bude:
                        [$$]
                        \\sgn(x^{(p)}\\cdot W)=\\sgn(y^{(p)} \\cdot (x^{(p)} \\cdot x^{(p)}) + \\sum_{l\\neq p}y^{(l)} \\cdot (x^{(l)} \\cdot x^{(p)}))
                        [/$$]<br>

                        Protože [$]x^{(p)} \\cdot x^{(p)} > 0[/$]:
                        [$$]
                        \\sgn(x^{(p)} \\cdot W)=\\sgn(y^{(p)} + \\sum_{l\\neq p}y^{(l)} \\cdot \\frac{x^{(l)} \\cdot x^{(p)}}{x^{(p)} \\cdot x^{(p)}})
                        [/$$]

                        Takže chci, aby absolutní hodnota všech složek
                        [$]\\sum_{l\\neq p}y^{(l)} \\cdot \\frac{x^{(l)} \\cdot x^{(p)}}{x^{(p)} \\cdot x^{(p)}}[/$] byla menší než 1.
        -
                uuid: 88bd1b9f-4b04-4351-982d-c37c4c467862
                front: Rekurentní asociativní paměť versus bez zpětné vazby
                back: >
                        Sféry vlivu atraktorů jsou v rekurentní větší. Ale zase rekurentní asociativní paměť má menší kapacitu.
        -
                uuid: bfd153d4-6451-41a1-80d9-8ba16a67a76f
                front: Maximální kapacita autoasociativní Hebbovské sítě podle dimenze vstupního vektoru
                back: >
                        [$]m\\sim 0.18n[/$], kde [$]n[/$] je dimenze vstupu, [$]m[/$] je počet uložených vzorů.
                        Předpokládáme nekorelované vzory.
        -
                uuid: 3c69131b-45db-4fbb-8a83-e4f11a7eb932
                front: Maximální kapacita autoasociativní Hebbovské sítě podle dimenze vstupního vektoru -- je to m=0.18n, jak se to odvodí?
                back: >
                        Chceme pro každý bit [$]i[/$] mít tohle menší než 1:
                        [$]
                        \\frac{1}{n} \\sum_{l\\neq p} x_i^{(l)}(x^{(l)} \\cdot x^{(p)})
                        [/$]

                        Tedy dostaneme [$]m\\cdot n[/$] náhodných hodnot, očekávaná hodnota je 0 (bipolární).

                        Součet má binomické rozdělení, pro aproximujeme ho normálním rozdělením s [$]\\sigma = \\sqrt{m/n}[/$].
                        Mrkneme se do tabulek, jak velký je interval že součet vyjde uvnitř [$]\\pm 1[/$]. Strčíme do toho, že chceme pravděpodobnost chyby [$]0.01[/$].
                        Dostaneme [$]m\\sim 0.18n[/$].
        -
                uuid: 7e9cb342-4b25-42d6-b8c5-1783fb56fc8d
                front: Proč můžeme chtít použít pseudoinverzní matici místo Hebbovského učení?
                back: Protože vzory můžou být korelované, takže nevhodné pro přímé Hebbovské učení s korelační maticí.
        -
                uuid: 0fd9a24f-e224-49fe-bb34-367ca7cc0625
                front: K čemu je pseudoinverzní matice?
                back: >
                        Hledáme váhovou matici pro asociační paměť že [$]XW=Y[/$].
                        Ale k [$]X[/$] nemusí existovat inverze, tak hledáme matici co minimalizuje [$]\\|XW-Y\\|^2[/$], to minimalizuje [$]W=\\hat{X}Y[/$]
        -
                uuid: 2d2d66de-68ad-4ced-bc71-877cf5c8949e
                front: Použití pseudoinverzní matice pro minimalizaci odchylky v single-layer BP síti
                back: >
                        Chci nulový gradient, který je podle váhy [$]w_{ij}[/$] rovný [$]-2\\sum_{p=1}^P (\\sum_{i=1}^n d_{j,p}-w_{ij} x_{i,p})x_{i,p}[/$]
                        Maticově: [$]WXX^T=DX^T[/$]. K [$]XX^T[/$] můžeme použít pseudoinverzi.<br>

                        Řešení může být víc: omezíme velikosti vah: [$]E=\\lambda\\sum_{i=1}^n \\sum_{j=1}^m w_{ij}^2[/$].<br>

                        Minimalizace přes parciální derivace: [$]W(XX^T+\\lambda I)=DX^T[/$]<br>

                        Pro [$]\\lambda>0[/$] existuje inverze k [$]XX^T+\\lambda I[/$],
                        tak to takhle vyřešíme pro [$]W[/$].<br>

                        Limitně pro [$]\\lambda\\rightarrow 0[/$]: [$]W = D\\hat{X}[/$].
                        Když je více řešení, tak tohle minimalizuje součet čtverců vah.
        -
                uuid: e2f01b17-6c02-4037-a66c-d326c5c33e74
                front: Výpočet pseudoinverzní matice přes vrstevnaté neuronové sítě. K čemu se hodí?
                back: >
                        Zkonstruuju síť, pro kterou střední kvadratická odchylka odpovídá [$]\|XW-Y\|^2[/$], backprop se bude snažit najít [$]W[/$] které to minimalizuje, tedy pseudoinverzi.
                        <br>
                        Hodí se k nalezení vah pro asociativní síť (BAM).
        # 07-bam-hopfield
        -
                uuid: 65863075-3e56-41dc-a24b-1a1d2e7492ba
                front: >
                        BAM: Co to je? Jak to vypadá?
                back: >
                        Bidirectional associative memory.<br>
                        Synchronní asociativní model, synapse jsou obousměrné.<br>
                        Je to typ rekurentní asociativní paměti.<br>
                        Snaží se dosáhnout stabilního stavu.<br>
                        Aktivační funkce [$]\\sgn[/$], bipolární kódování.
        -
                uuid: 957d2bad-5577-4979-b1a3-cf31aac81c16
                front: >
                        BAM: jak se dá rovnicemi vyjádřit stabilní stav?
                back: >
                        [$]y=\\sgn(xW) \\wedge x=\\sgn(Wy)[/$], snaží se najít po konečném množství iterací
        -
                uuid: dc4ecdf7-3448-4d47-8ffb-248a0df135d4
                front: Jak se učí BAMy? Co to znamená na jednom vzoru?
                back: >
                        Hebbovským učením.

                        Pro jeden vzor: [$]W=x^T y[/$], takže [$]y=\\sgn(xW)=\\sgn(x x^T y) = \\sgn(\\|x\\|^2 y)=y[/$],
                        to samé pro [$]x[/$].
        -
                uuid: be56641a-04d0-4f75-b00e-7a343e9a79dd
                front: Kdy by [$] (x_0,y_0)[/$] byl stabilní stav BAM? Jak se z toho odvodí energetická funkce BAMu?
                back: >
                        [$$]e^T:=W\cdot y_0[/$$]<br>

                        [$] (x_0,y_0)[/$] je stabilni stav, kdyz [$]\\sgn(e)=x_0[/$]
                        (skalarni soucin [$]x_0 \\cdot e^T[/$] by mel byt vetsi nez skalarni soucin jinych excitacnich vektoru a [$]x_0[/$])<br>

                        Energeticka funkce BAM: [$]E:=-x_0 W y_0^T[/$].<br>
                        Když [$]Wy_0[/$] je blíž k [$]x_0[/$], tak tenhle krok ji zmenší.<br>
                        Lokální minima odpovídají stabilním stavům.
        -
                uuid: f1216920-8f6e-4dc5-a34e-1e0911a9a5fb
                front: Zobecnění energetické funkce BAM pro neurony, kde přidáme práh
                back: >
                        Vektory [$]x[/$] transformujeme tak že přidáme jedničku na konec.
                        Jako bychom přidali do obou vrstev neuron s výstupem 1.

                        [$]E(x_i,y_i) = -\\frac{1}{2}x_i W y_i^T + \\frac{1}{2}\\theta_r y_i^T + \\frac{1}{2}x_i \\theta_l^T[/$]
        -
                uuid: 7fd399a9-7005-43f1-b5d2-e71db5da2b68
                front: Asynchronní BAM a konvergence
                back: >
                        Vyberu náhodný neuron a opravím jeho výstup podle jeho excitace.
                        Končím když jsem v stabilním stavu.

                        Asynchronní sítě konvergují, protože změna neuronu vždycky sníží energii a je konečně mnoho stavů.

                        To platí i pro sítě se synchronní dynamikou.
        -
                uuid: 29887485-c696-4ff1-a7f6-990fa86294c5
                front: Hopfieldova síť
                back: >
                        Bipolární neurony se skokovou přenosovou funkcí, propojené všechny se všemi.
                        Používá se na učení bez učitele a optimalizaci. Neodděluje vstupní a výstupní neurony.
        -
                uuid: 85cd9e4d-f569-4759-8632-3d89d8709ccb
                front: Učení Hopfieldových sítí
                back: >
                        [$$]w_{ij} = \\sum_{s=1}^m x_{i}^{(s)} x_j^{(s)}
                        [/$$]
                        Když [$]i=j[/$], tak je to 0.
        -
                uuid: 2fe35573-4e3d-491a-94ae-91aef75a21cc
                topic: Hopfieldovy sítě
                front: Průběh výpočtu Hopfieldovy sítě
                back: >
                        Vyber náhodný neuron k aktualizaci.
                        Opakuj dokud se neustálí.
                        Výstup je pak vzor, který se nejvíc podobal předloženému vektoru.
        -
                uuid: f1828634-701b-4d8d-84bd-3f624d7aa561
                topic: Hopfieldovy sítě
                front: Přibližná kapacita na vzory
                back: >
                        [$]m<0.15n[/$], kde [$]n[/$] je počet neuronů
        -
                uuid: d4fbf83b-6cc0-46a4-81b5-0ae20bd8f800
                topic: Hopfieldovy sítě
                front: Tvar vektoru potenciálů. Jak vypadá perturbační člen? Kdy je stav stabilní?
                back: >
                        [$$]\\xi = x_1 \\cdot W = x_1 \\cdot (x_1^T x_1 + \\ldots + x_m^T x_m - mI) =

                        = (x_1 x_1^T) x_1 + (x_1 x_2^T) x_2 + \\ldots + (x_1 x_m^T) x_m - m x_1 I[/$$]

                        Přitom: [$]x_1 x_1^T = n[/$], označíme si [$]x_1 x_2^T =: \\alpha_{12}[/$].

                        Pak: [$]\\xi = (n-m)x_1 + \\sum_{j=2}^m \\alpha_{1j}x_j[/$]

                        Poslední věci se říká *perturbace*.

                        Stav je stabilní, když [$]m<n[/$] a perturbace je malá, pak [$]\\sgn(\\xi)=\\sgn(x_1)[/$]. Chceme tedy malý počet ortogonálních vzorů.
        -
                uuid: 8757a11c-941d-40bb-95d4-e982a78f62d5
                topic: Asynchronní BAM sítě
                front: Synchronní a asynchronní BAM a maticové stabilní stavy
                back: >
                        Libovolná reálná matice vah má bidirektivní stabilní bipolární stavy.
        -
                uuid: e6c91db6-f75c-4dc9-9ca0-78498b24e83e
                topic: Hopfieldovy sítě
                front: Nutná podmínka ke stabilnímu řešení
                back: Symetrická váhová matice s nulovou diagonálou, asynchronní dynamika
        -
                uuid: 7f3b7147-4c97-493b-b828-dfbbd17d20e6
                topic: Hopfieldovy sítě
                front: Energetická funkce, s prahovými neurony
                back: >
                        [$$]E(x)=-\\frac{1}{2}x^T Wx[/$$]<br>

                        S prahy: [$] + \\theta^T x[/$]
        -
                uuid: 2c7f716b-6ada-42d0-a6dd-683b2db5f565
                front: K čemu se hodí perceptronové učení Hopfieldových sítí?
                back: Někdy nejde Hebbovským učením najít správné váhy, i když existují. Crosstalk může být moc velký.
        -
                uuid: 54949644-7a63-4284-a567-4f76a73d9c11
                front: Vzorečky pro perceptronové učení Hopfieldových sítí
                back: >
                        Máme mít stabilní stavy. Tedy:

                        Neuron 1: [$]\\sgn(x_1) (0+x_2 w_{12} + \\ldots + x_n w_{1n}-\\theta_1) > 0[/$],
                        Neuron 2: [$]\\sgn(x_2) (x_1 w_{21}+0 + \\ldots + x_n w_{2n}-\\theta_2) > 0[/$],
                        atd.

                        Zapíšeme jako:
                        Neuron 1: [$]\\sgn(x_1) z_1 \\cdot v > 0[/$],
                        Neuron 2: [$]\\sgn(x_2) z_2 \\cdot v > 0[/$],
                        atd.

                        Každý vzor transformujeme do [$]n[/$] "pomocných" podvzorů, co odpovídají všem rovnicím co máme splnit.
        -
                uuid: e4208254-d1d3-4403-9d1f-68f711e471f8
                front: Použití Hopfieldova modelu na řešení optimalizačních úloh - Multiflop
                back: >
                        Úloha: chceme mít aktivní právě jeden neuron z binárních<br>

                        Cíl: [$]E(x_1,\\ldots x_n)=(\\sum_{x_i} - 1)^2[/$]<br>

                        Podobně se dá použít na 8-queens, TSP.
        -
                uuid: a6919724-8154-4f96-a8ff-83bf8c0c004e
                front: Stochastické modely Hopfieldových sítí - dvě strategie
                back: >
                        A. spojitý model -- sigmoidální přenosová funkce. zvětší počet cest k řešení co je globální optimum.

                        B. omezení lokálních minim -- "zašuměná dynamika". simulované žíhání, Boltzmannův stroj.

                        obě brání uváznutí v lokálním minimu.
        -
                uuid: cbd26dea-0d20-44b5-9a73-4983d8a7c9dd
                front: Spojitý model Hopfieldovy sítě
                back: >
                        Excitace neuronů: [$]u_i[/$]

                        Aktivace neuronů: [$]x_i = \\sigma(u_i) = \\frac{1}{1+e^{-u_i}}[/$]

                        Pomalá změna excitace v čase, kde [$]\\gamma>0[/$] je adaptační parametr:
                        [$$]
                        \\frac{d u_i}{dt} = \\gamma(-u_i+\\sum_{j=1}^n w_{ij}x_j) = \\gamma(-u_i+\\sum_{j=1}^n w_{ij}s(u_j))
                        [/$$]

                        Výpočet: diskrétně aproximovat derivaci a přičíst

                        Asynchronni dynamika vede do rovnovazneho stavu.
        -
                uuid: 08329897-2aea-417a-b4dd-3b6e5457f557
                topic: Zespojitěné Hopfielovy sítě
                front: Energetická funkce spojitého modelu Hopfieldovy sítě
                back: >
                        [$$]E = -\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} x_i x_j + \\sum_{i=1}^n \\int_{0}^{x_i} s^{-1}(x) dx[/$$]
        -
                uuid: d7e6e3cc-da15-4d8f-8c0b-f362e006f319
                topic: Zespojitěné Hopfielovy sítě
                front: Proč konverguje do lokálního optima?
                back: >
                        Zderivuj energii podle času. Dostaneš:
                        [$$]dE/dt = -\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} \\frac{dx_i}{dt} x_j + \\sum_{i=1}^n \\sigma^{-1}(x_i) \\frac{dx_i}{dt}[/$$]<br>

                        Síť je symetrická a [$]u_i=\\sigma^{-1}(x_i)[/$], tak:
                        [$$]dE/dt=-\\sum_{i=1}^n \\frac{dx_i}{dt}(\\sum_{j=1}^n w_{ij}x_j - u_i)[/$$]<br>

                        Přitom [$]du_i/dt = \\gamma(\\sum_{j=1}^n w_{ij}x_j - u_i)[/$], tedy [$]dE/dt=-\\frac{1}{\\gamma}\\sum_{i=1}^n \\frac{dx_i}{dt}\\frac{du_i}{dt}[/$]<br>

                        Dosaď [$]x_i=s(u_i)[/$]: [$]dE/dt = -\\frac{1}{\\gamma}\\sum_{i=1}^n \\sigma'(u_i) (\\frac{du_i}{dt})^2[/$]<br>

                        Protože [$]\\sigma'(x_i)>0[/$] (sigmoida roste) a [$]\\gamma>0[/$], tak [$]dE/dt\\leq 0[/$].
        -
                uuid: aeb40e17-72bf-40d6-bd22-1b8eef1daa9d
                front: Simulované žíhání
                back: >
                        Pokud změna sníží energii, proveď ji.
                        Pokud ji zvýší o [$]\\Delta E[/$], přijmi s [$]p=\\frac{1}{1+\\exp(\\Delta E/T)}[/$] kde [$]T[/$] je teplotni konstanta.
                        Pro velké [$]T[/$] je [$]p\\simeq 1/2[/$].
        -
                uuid: a1609cff-c797-49be-b60e-c4ad967dd11d
                topic: Boltzmannův stroj
                front: Architektura, pravděpodobnost aktualizace
                back: >
                        Hopfieldova síť<br>
                        Aktualizace stavu [$]x_i[/$] na 1: pravděpodobnost [$]p_i[/$], jinak na 0<br>

                        [$$]p_i = \\sigma(\\frac{\\sum_{j=1}^n w_{ij} x_j - \\theta_i}{T})[/$$]
        -
                uuid: 2626be18-48e4-4d6e-9167-678859a66e4e
                topic: Boltzmannův stroj
                front: Energetická funkce
                back: >
                        [$$]E=-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}x_i x_j + \\sum_{i=1}^n \\theta_i x_i[/$$]
        -
                uuid: 08d6bd3e-bfe2-4252-8662-919cdacb71e1
                front: Vztah mezi Boltzmannovým strojem a Hopfieldovou sítí
                back: >
                        Boltzmannův stroj aproximuje Hopfieldovu síť, najde lokální minimum energetické funkce.

                        Nezůstane v jediném stavu. Pro velké teploty může projít celý stavový prostor.

                        Pokud budeme správně snižovat teplotu, s [$]p=1[/$] najde globální optimum.
        # 08-unsupervised
        -
                uuid: 0d486c65-3912-4a06-af42-148b417569af
                front: Jaký vztah mezi neurony se používá k učení bez učitele?
                back: >
                        Inhibice: neurony soutěží o rozpoznání vzoru. "Winner takes all."
        -
                uuid: d9e8f239-ce85-41ae-9756-3f8189b2893c
                front: Síť na kompetiční učení bez učitele
                back: >
                        Neurony počítají Euklidovskou vzdálenost.
                        Vyhraje nejbližší, a přes laterální spoje potlačí ostatní.
                        Vítěz se posune o [$]\\alpha[/$] směrem k vstupu.
        -
                uuid: 38bba9b3-1dc3-4b0c-8dc1-609cebc0f004
                front: Síť na kompetiční učení bez učitele - vhodná inicializace vah
                back: >
                        Váhy budou náhodné vzory z datasetu.
        -
                uuid: 521ab663-7660-4b7b-bba7-65f1acce4255
                front: Problémy v síti na kompetiční učení bez učitele
                back: >
                        Mrtvé neurony: v Kohonenově mapě pomáhá mřížka
        -
                uuid: 1fdba122-da33-484b-aca0-bec67d03b8c1
                markdown: false
                front: Energetická funkce sítě na kompetiční učení bez učitele s jedním neuronem. Kde je optimum? Jak se to dokáže?
                back: >
                        [$$]E_X(w)=\sum_{i=1}^m \|x_i-w\|^2[/$$]<br>

                        Dá se ukázat, že optimum je v těžišti:<br>

                        [latex]
                        \begin{align*}
                        E & = & \sum_{i=1}^m \|x_i-w\|^2 = \sum_i\sum_j(x_{ij}-w_j)^2= \\
                          & = & \sum_i \sum_j (x_{ij}^2 - 2x_{ij}w_j + w_j^2) = \\
                          & = & m\left( (\sum_j w_j^2) - \frac{2}{m}\sum_j w_j (\sum_i x_{ij}) \right) + \sum_i \sum_j x_{ij}^2 = \\
                          & = & m\sum_j (w_j^2 - \frac{2}{m}\sum_j(\sum_i x_{ij}) +
                                \frac{1}{m^2}(\sum_i x_{ij})(\sum_i x_{ij}))
                                \underbrace{- \frac{1}{m}\sum_{j=1}^n (\sum_i x_{ij})(\sum_i x_{ij}) + \sum_i \sum_j x_{ij}^2}_{=K} = \\
                          & = & m(\sum_j (w_j - \frac{1}{m}\sum_i x_{ij})^2) + K = \\
                          & = & m\|w-x^{ * }\|^2 + K
                        \end{align*}
                        [/latex]

        -
                uuid: 2072bd8e-458a-45e3-953a-8c2bab6198d2
                front: >
                        [$]k[/$]-means algoritmus
                back: >
                        Update: nastav středy jako těžiště jejich shluků; Přenastav příslušnosti ke shlukům
        -
                uuid: c16c1976-bf8b-40a8-a51e-d9dae9a8f694
                front: Jak se pro kompetiční učení bez učitele předzpracuje vstup?
                back: Vstupní vektory se znormují
        -
                uuid: c92483e6-602a-45a3-865a-9784a175cbb3
                front: Různá pravidla učení pro sítě na kompetiční učení bez učitele
                back: >
                        Aktualizace pomocí parametru učení: [$]\\Delta w = \\eta x_j[/$]<br>

                        Diferenční: [$]\\Delta w = \\eta (x_j - w)[/$]<br>

                        Dávková aktualizace
        -
                uuid: 6e149cef-86c7-4d3a-8eef-7244d308200a
                front: Stabilní rovnovážný stav v kompetičním učení - jaká je postačující podmínka?
                back: >
                        Chceme jasně ohraničené shluky, co se nepřekrývají a nejsou moc rozsáhlé.<br>

                        Svazek := [$]\\{x = \\alpha_1 p_1 + \\ldots \\alpha_m p_m : \\alpha_1 \\in \\R^+\\}[/$]<br>

                        Průměr svazku: [$]\\varphi = \\sup\\{\\arccos (a\\cdot b)|\\|a\\|=\\|b\\|=1, a,b\\in S\\}[/$]<br>

                        Postačující podmínka pro stabilitu: úhlový průměr svazků je menší než jejich vzájemná vzdálenost (jakmile se střed shluku dostane do svazku, už tam dostane)
        -
                uuid: f3d9f7e9-bc7c-43a1-b5c5-a652ebd0776d
                topic: PCA
                front: Co to je? Co dělá?
                back: >
                        Principal component analysis.<br>

                        Hlavní komponenta je ta, co vysvětluje (ve vystředěných a normalizovaných datech)
                        nejvíce rozptylu.<br>

                        Ostatní jsou na ni ortogonální.

        -
                uuid: 24d127c4-e258-4169-b08c-297848fc76cb
                topic: PCA
                front: Obecný algoritmus na výpočet
                back: >
                        Hlavní komponenta: maximalizuje [$]\\frac{1}{m} \\sum_{i=1}^m (w \\cdot x_i)^2[/$]<br>
                        Druhá: odečteme od vektorů jejich projekci na první a pokračujeme
        -
                uuid: a7bde697-35ba-408c-aacd-ee4db75e8e9a
                topic: Ojův algoritmus
                front: Co je jeho cíl?
                back: Hledá hlavní komponentu v matici pro PCA.
        -
                uuid: 81f631a0-b983-43e7-b63c-facf062cfed2
                topic: Ojův algoritmus
                front: Co předpokládá?
                back: Těžiště dat je v počátku
        -
                uuid: 11a4da75-7474-41b3-b866-bd1ada9f1198
                topic: Ojův algoritmus
                front: Jak probíhá?
                back: >
                        Náhodně inicializuje vektor [$]w[/$], zvolí parametr učení [$]\\gamma\\in(0;1][/$]<br>

                        Náhodně vyber [$]x[/$], spočítej [$]\\Phi=x\\cdot w[/$],
                        nový váhový vektor je [$]w+\\gamma\\Phi(x-\\Phi w)[/$].
                        Zmenši [$]\\gamma[/$] a opakuj.<br>

                        Váhový vektor se "automaticky normalizuje".
        -
                uuid: b305d565-9d5b-45ca-a948-b399db50fb5e
                topic: Ojův algoritmus
                front: Konvergence a důkaz
                back: >
                        Idea: začne-li Ojův algoritmus ve svazku, bude v něm oscilovat, ale neopustí ho.<br>
                        Pro [$]\\|w\\|=1[/$] je [$]\\Phi=x\\cdot w[/$] délka projekce [$]x[/$] na [$]w[/$]<br>

                        Vektor [$]x-\\Phi w[/$] je kolmý na [$]w[/$].<br>

                        Iterace přitahují [$]w[/$] k vektorům ze shluku X.<br>

                        Pokud je délka [$]w[/$] blízko 1, umístí se [$]w[/$] v centru shluku.<br>

                        Dále to chce dokázat, že Ojův algoritmus automaticky normuje [$]w[/$].
        -
                uuid: 6192e830-b4e3-4865-937b-7b81bca3cc1f
                front: Problémy Ojova algoritmu, zobecnění
                back: >
                        "Řidké" shluky, příliš velké rozdíly v délce vstupních vektorů.
                        Dá se zobecnit, aby najednou počítal více nejdůležitějších příznaků.
        # 09-kohonen-hybrid
        -
                uuid: adc2c3cf-9d46-4ac2-aff4-e616686aee7c
                topic: Kononenova mapa
                front: >
                        K čemu je? Jaké má parametry?
                back: >
                        Učení bez učitele.<br>
                        Najdi topologické okolí nejbližšího neuronu a taky ho uprav.<br>
                        Funkce laterální interakce: [$]\\Phi(i,k)[/$], například "funkce mexického klobouku"<br>
                        Vigilační koeficient [$]\\alpha\\in(0;1)[/$] určuje plasticitu, klesá v čase.
        -
                uuid: 964a216b-1159-4e20-840a-e79d82a64e67
                topic: Kononenova mapa
                front: Co by mělo platit v Kohonenově mapě pro vigilační koeficient?
                back: >
                        [$$]\\sum_{t=1}^\\infty \\alpha(t)=\\infty \\wedge \\sum_{t=1}^\\infty \\alpha^2(t) < \\infty[/$$]
        -
                uuid: 6d368fb6-973e-465d-8a2d-30bea1c20260
                topic: Kononenova mapa
                front: >
                        Kostra důkazu: Jeden neuron Kohonenovy mapy na intervalu [$][a;b][/$] se ustálí ve středu
                back: >
                        Adaptační pravidlo: [$]w_n\\leftarrow w_{n-1}+\\eta(x-w_{n-1})[/$], kde [$]x[/$] je náhodně zvolené z [$][a,b][/$].
                        Pro [$]\\eta\\in(0;1)[/$] nemůže [$]w[/$] opustit [$][a,b][/$]<br>

                        Ocekavana hodnota [$]\\E[dx/dt][/$] musi byt 0, jinak by mohlo byt [$]\\E[x]\\notin [a;b][/$].<br>

                        [$$]\\E[dx/dt]=\\eta(\\E[x]-\\E[w])=\\eta((a+b)/2 - \\E[x]) = 0 \\longrightarrow \\E[w]=(a+b)/2[/$$]
        -
                uuid: 69a572dd-fa8e-40b6-9367-29d8a18c7d99
                topic: Kononenova mapa
                front: Varianty učících algoritmů pro učení s učitelem pro Kohonenovy mapy
                back: >
                        Pro učení s učitelem:
                        <ul>
                        <li>
                        LVQ1: [$]x[/$] by mělo patřit ke stejné třídě jako
                        nejbližší [$]w_i[/$].<br>

                        Vezmi nejbližší neuron, když je klasifikovaný stejně,
                        přibliž ho, kryž různě, oddal ho. S ostatními neurony
                        nehýbej.
                        <li>
                        LVQ2.1: Adaptuj dva nejbližší sousedy současně. Jeden
                        z nich musí patřit ke správné třídě, druhý k nesprávné.
                        Když je [$]x[/$] z okenka mezi [$]w_i[/$], [$]w_j[/$] a
                        ty neurony maji ruzne tridy, tak spravny neuron prisun
                        a spatny odsun.<br>
                        Okenko: [$]\\min(\\frac{d_i}{d_j}, \\frac{d_j}{d_i})>\\frac{1-s}{1+s}[/$],
                        kde [$]s[/$] je sirka okenka, treba [$]0.2[/$] az [$]0.3[/$]
                        <li>
                        LVQ3: Plus: jestlize [$]w_i[/$] i [$]w_j[/$] patri do
                        stejne tridy, tak je prisun
                        o [$]\\varepsilon \\alpha(t)[x(t)-w_{i/j}(t)][/$], kde [$]0.1\\leq\\varepsilon\\leq 0.5[/$]
                        </ul>
        -
                uuid: 56e732ce-9373-44b0-9d81-7c6cb3d4561a
                front: Sítě se vstřícným šířením (counterpropagation)
                back: >
                        Ma dve vrstvy: Kohonenovskou a Grossbergovskou (adaptuje vahy jen pro vitezne neurony z Kohonenovy vrstvy).
                        Pouziva se jako heteroasociativni pamet.<br>

                        1. Vyber nejblizsi neuron v Kohonenovse vrstve.<br>
                        2. Aktualizuj vahy v Kohonenovske vrstve<br>
                        3. Uprav vahy mezi "vitezem" [$]c[/$] z Kohonenovy vrstvy a neurony Grossbergovske vrstvy, aby vystup odpovidal vic ocekavane odezve [$]d[/$]:<br>

                        [$$]w_{cj} \\leftarrow (1-\\beta)w_{cj} + \\gamma z_c d_j[/$$]<br>

                        Kde [$]\\beta>0, \\gamma>0, z_c[/$] je aktivita vitezneho neuronu Kohonenovske vrstvy
        -
                uuid: e8b48098-6b46-4500-8fc9-d7e816888ce9
                topic: RBF sítě
                front: Definice RBF funkce a co ta zkratka znamená
                back: >
                        Radial Basis Functions.<br>
                        [$$]RBF(x,w,\\sigma) := \\exp\\{-\\frac{(x-w)^2}{2\\sigma^2}\\}[/$$]
        -
                uuid: fa514eee-0607-4206-af82-7df4be2c1dd3
                topic: RBF sítě
                front: Jakou mají architekturu?
                back: >
                        Vstup => Kohonenovská vrstva s Gaussovskou přenosovou funkcí => lineární asociátor => skalární výstup
        -
                uuid: b59d1818-0ec4-4305-8bb3-f8a90582d92a
                topic: RBF sítě
                front: Jak počítá?
                back: >
                        Neuron [$]j[/$] počítá výstup
                        [$]g_j(x)=RBF(x, w_j, \\sigma_j) / \\sum_k RBF(x, w_k, \\sigma_k)[/$].<br>

                        [$]\\sigma_i[/$] jsou konstanty: například nastavené podle vzdálenosti mezi váhovým vektorem a jeho nejbližším sousedem<br>

                        Z RBF vrstvy vedou váhy [$]z_1\\ldots z_m[/$].<br>
                        Ty jdou nastavit třeba zpětným šířením.
        -
                uuid: af99f342-72d2-4801-a35a-bbfb0fdde854
                topic: RBF sítě
                front: Jak jdou nastavit váhy [$]z_i[/$] z Kohonenovských neuronů k lineárnímu asociátoru?
                back: >
                        [$$]
                        E=\\frac{1}{2}\\sum_p (\\sum_{i=1}^n g_i(x_p)z_i - d_p)^2
                        [/$$]<br>

                        [$$]
                        \\Delta z_i \\simeq -\\partial E/\\partial z_i = \\gamma g_i(x) (d - \\sum_{j=1}^n g_j(x) z_j)
                        [/$$]
        -
                uuid: a4eb55d5-10de-44d4-8d2f-5b91d7f334cd
                topic: ART
                front: ART sítě -- architektura, jaké je kódování
                back: >
                        Dvě vrstvy: vstupní a výstupní.<br>

                        Vstupy jsou binární. Učí se bez učitele.<br>

                        Spojení: vstupy => výstupy, výstupy => vstupy, výstupy mezi sebou (ale bez smyček z neuronu zpět do něj)<br>

                        Laterální inhibice na určení výstupního neuronu s maximální odezvou.
                        Zpětná vazba z výstupu ke vstupu je k porovnání skutečné podobnosti s rozpoznaným vzorem.<br>

                        Má mechanismus pro vypnutí výstupního neuronu s maximální odezvou.<br>

                        Problém: i při troše šumu začne ukládat moc vzorů
        -
                uuid: 179f5bf7-0dd8-42d5-b161-c607274db0de
                topic: ART
                front: Značení vah a parametrů
                back: >
                        [$]t_{ij}(0) = 1[/$] (váhy z výstupu do vstupu -- vzor specifikovaný výstupním neuronem [$]j[/$])<br>

                        [$]b_{ij}(0)=1 / (1+N)[/$] (váhy ze vstupu do výstupu)<br>

                        [$]\\rho\\in[0;1][/$] -- jak blízko musí být vstup k uloženému vzoru, aby byl ve stejné kategorii.<br>

                        Výstupy výstupních neuronů: [$]\\mu_j[/$]
        -
                uuid: ae99b9ea-f197-41ae-a9ca-e08804981b66
                topic: ART
                front: Učící algoritmus
                back: >
                        Předlož nový vstup.<br>

                        Spočítej výstupy výstupných neuronů: [$]\\mu_j = \\sum_{i=0}^{N-1} b_{ij}(t) x_i[/$]<br>

                        Vyber nejlépe odpovídající vzor: [$]\\mu_{j^{ * }} = \\max_j\\{\\mu_j\\}[/$]<br>

                        Test bdělosti: [$]\\|x\\|=\\sum_i x_i, \\|T\\cdot x\\|=\\sum_i t_{ij} \\cdot x_i[/$]
                        Když [$]\\frac{\\|T\\cdot x\\|}{\\|x\\|}<\\rho[/$], dočasně nastav výstup [$]j^{ * }[/$] na nulu, znova vyber nejlepší neuron (aniž se tenhle účastní).<br>

                        Aktualizace nejlépe odpovídajícího neuronu:
                        [$]t_{ij^{ * }}(t+1) = t_{ij^{ * }}(t) \\cdot x_i[/$],
                        [$]b_{ij^{ * }}(t+1) = \\frac{t_{ij^{ * }} \\cdot x_i}{0.5 + \\sum_{k=0}^{N-1} t_{kj^{ * }}(t) \\cdot x_k}[/$]<br>

                        Odmraž neurony a opakuj.
        -
                uuid: bfffc115-79e0-4ac9-afe0-4c70be3876b1
                front: Kaskádová korelace, algoritmus učení
                back: >
                        Začnu s přímým propojením vstupů na výstup.<br>
                        Postupně přidávám skryté neurony.<br>
                        Vstupy nového neuronu jsou propojeny se všemi předchozími vstupy a výstupy dřívějších neuronů.

                        1. Vezmi existující síť, adaptuj přes QuickProp. Když je dost malá chyba, ukonči učení.<br>
                        2. Přidej neuron, který maximalizuje korelaci svého výstupu s chybou na výstupu sítě. V další iteraci zmrazím váhy jeho vstupů, doučím váhu jeho výstupů.<br>

                        Chceme maximalizovat [$]S[/$]:
                                [$$]S:=|\\sum_{i=1}^p (V_i-\\overline{V})(E_i-\\overline{E})|[/$$]<br>

                        [$]V_i[/$]: výstup přidávaného neuronu pro vzor [$]i[/$], [$]\\overline{V}[/$]: průměrný výstup přidávaného neuronu, to samé chyba<br>

                        [$]\\partial S/\\partial w_k = \\sum_p \\delta_p I_{pi}[/$],
                        kde:
                        <br>
                        [$]\\delta_p = \\sum_o \\sigma_o(e_{po}-\\overline{e_j})f'_p

                        <br>
                        [$]\\sigma_o[/$] je znaménko korelace mezi přidávaným neuronem a reziduální chybou na výstupu [$]o[/$]<br>
                        [$]f'_i[/$] je derivace přenosové funkce ve výstupní jednotce<br>
                        [$]I_{pi}[/$] je [$]i[/$]-tý vstup přidávaného neuronu pro vzor [$]p[/$]
        # 10-genetic
        -
                uuid: 8949770d-17e2-4828-9311-0222a69d69b6
                front: Použití genetických algoritmů v učení NS
                back: >
                        Dají se použít normální strategie - crossover a mutace (na malých sítích) -- jako optimalizace spojitých funkcí.<br>

                        Algoritmus NEAT.
        -
                uuid: 00546be9-75f8-49af-b562-ff8f95712268
                front: Jednoduchý genetický algoritmus
                back: >
                        Populace; binární (nebo jiný) genotyp; fitness funkce<br>

                        1) Spočítej ohodnocení<br>
                        2) Selektivní reprodukcí vyber novou populaci<br>
                        3) Proveď křížení<br>
                        4) Mutuj chromosomy<br>

                        Reprodukce ruletou.
        -
                uuid: 89dee31c-00c2-4498-b510-539194330190
                front: Problémy jednoduché ruletové selekce a řešení
                back: >
                        Když mají všichni jedinci stejné ohodnocení, nebo jich pár silně dominuje.

                        Řešení: škálování: zvětšit nebo zmenšit rozdíly; selekce podle pořadí (a jemu úměrná ruletová hmotnost)
        -
                uuid: 4506dd19-9727-46f9-b572-e35b1d8a0441
                front: Selekce s ořezáváním
                back: >
                        Jedince uspořádej podle dobrosti, okopíruj nejlepších [$]X[/$] [$]Y[/$]-krát
        -
                uuid: c786470f-76d9-4a68-b591-592478196b7c
                front: Turnajová selekce
                back: >
                        Vezmi dva jedince; ten, co vyhraje (podle nějakého parametru) se zkopíruje
        -
                uuid: 003d14ee-206b-4c43-aa2c-bf70d981622f
                front: Elitismus
                back: >
                        Zajisti že několik nejlepších jedinců se vždy zkopíruje
        -
                uuid: 9b287a54-9712-4d0f-bbd5-fbec603f18b0
                front: Typy křížení sekvence
                back: >
                        Jednobodová, vícebodová, uniformní
        -
                uuid: 82506d9b-4246-4b07-9138-8567d2cf5068
                front: Schema - kolik jich je pro délku? Kolik je genomů?
                back: >
                        Schema: genom, kde jsou 0/1/?; schemat je [$]3^l[/$].
                        Genomů je [$]2^l[/$].
                        Schemata se považují za základní blok evoluce.
        -
                uuid: 1e6c2269-d808-4569-a3b7-5583055ef63f
                front: Holland 1975 - implicitní paralelismus GA
                back: >
                        GA zpracuje v jednom kroku až [$]N^3[/$] schemat, když používá [$]N[/$] řetězců.
        -
                uuid: ec60fcee-7214-47f9-ad64-5737c20f9c0d
                front: Definice - řád schematu
                back: >
                        Počet pevných pozic
        -
                uuid: 3ac069af-1e34-404b-95be-06ae9e9349e8
                front: Definice - délka schematu
                back: >
                        Vzdálenost prvního a posledního zafixovaného písmena
        -
                uuid: a8406dd6-e66f-4bfd-b52a-31eb0f81eebf
                front: Věta o schematech
                back: >
                        [$$]m(H, t+1) \\geq m(H, t) \\cdot \\frac{f(H)}{\\overline{f}} \\cdot [1 - p_e \\frac{\\delta(H)}{\\ell - 1} - o(H) \\cdot p_m][/$$]

                        [$]m(H,t)[/$]: počet řetězců v populaci odpovídajících schematu<br>
                        [$]f(H)[/$]: průměrná fitness jedinců odpovídajících schematu<br>
                        [$]p_e[/$]: křížení nastává náhodně<br>
                        [$]p_m[/$]: pravděpodobnost mutace na jedné pozici<br>
                        [$]o(H)[/$]: řád schematu<br>

                        Pro malé [$]p_m[/$]: [$] (1-p(m))^{o(H)} \\approx 1-o(H) \\cdot p_m[/$]
        -
                uuid: d0369892-a5b7-4b6f-b54f-be61f6d28fed
                front: Kdy může být lepší použít GA než backprop?
                back: Například když nemám explicitní derivovatelnou chybovou funkci, ale mám jenom fitness.
        -
                uuid: 31b5482b-5f69-43ac-876a-921d5e89723d
                front: Algoritmus NEAT - přibližně
                back: >
                        Neuroevolution of Augmenting Topologies<br>

                        Síť: seznam hran, každá hrana má informace o vrcholech,
                        vahách a "rodné číslo".<br>

                        Kříží se jen hrany se stejnými "rodnými čísly", zbytek
                        se přenáší beze změn. (Tedy křížím jen hrany se stejným evolučním původem.)<br>

                        Na vektorech hran s rodnými čísly se definuje podobnost.
                        Při evoluci jsou podobné sítě zahrnuty do stejného
                        druhu, fitness je relativní vzhledem k druhu.
                        (Tím se chrání nové topologie než se jim dovyvinou váhy.)<br>

                        Každý neuron taky má rodné číslo.<br>

                        Mutace: přidání spoje, přidání neuronu.
                        Když jsou jedinci moc daleko, jsou v různých druzích.
        -
                uuid: 0f735c39-2b8a-4a1c-9db6-4c3d532834dc
                front: Evoluce architektury
                back: >
                        Genotyp: kodovani prepisovacich pravidel; jejich vystup je binarni matice spoju mezi neurony, vahy se douci BP
        -
                uuid: 33a61f5b-ffdd-4e9b-a83f-38fb18a94d25
                front: Algoritmus SANE
                back: >
                        Jedna skrytá vrstva.<br>
                        Vyvíjí se jednotlivé skryté neurony.<br>
                        Fitness neuronů je fitness všech testů, kde byl použit.<br>
                        Každý neuron použijeme aspoň 10x.<br>
                        Vítězi jsou nejužitečnější neurony.
        -
                uuid: efdd20d0-8450-450b-b812-bfe059c7fc3e
                front: Algoritmus ESP
                back: >
                        Enforced Sub-Populations<br>

                        Vyvijeji se jednotlive neurony, aby spolupracovaly.
                        Dopredu se stanovi pocet druhu, vyvoj probiha v ramci jednoho druhu.<br>

                        Jeden (skryty) neuron = jedna sub-populace.
                        Vyviji se pevna topologie.
                        Daji se vyvijet i rekurentni site.
        -
                uuid: 4831c1e3-be4b-4d5a-869c-9924893fd821
                front: Algoritmus CoSyNE
                back: >
                        Cooperative Synapse NeuroEvolution<br>

                        Vahy pro jednu synapsi jsou v samostatne subpopulaci.
        # ATNS modular
        -
                uuid: c5fa0a87-3c18-47f1-ba41-4bc20bebc629
                front: Dva druhy robustnosti NS a jak se jich dosahuje
                back: >
                        Vzhledem ke ztrátě neuronu: přidám robustnostní člen
                        do chybové funkce; prořezání a duplikace: zdvojení těch
                        neuronů, jejichž ztráta způsobí největší odchylku na
                        výstupu sítě<br>

                        Vzhledem k drobným odchylkám vstupu: zkonstruuj
                        [$]\\varepsilon[/$]-ekvivalentní síť co je robustnější
        -
                uuid: 6302a3ec-2753-4bd5-a9a2-10401f2fc5b9
                front: >
                        [$]\\varepsilon[/$]-ekvivalence neuronových sítí
                back: >
                        Jsou [$]\\varepsilon[/$]-ekvivalentní, když pro všechny vstupy [$]\\|y_{B_1} - y_{B_2}\\|\\leq\\varepsilon[/$]
        -
                uuid: 13d67716-cd49-41a6-8b73-a17101a23771
                front: Robustnostní chybová funkce -- různé varianty, jejich různé následky
                back: >
                        [$]y_{i,-k}[/$]: výstup [$]i[/$] když zruším neuron [$]k[/$]<br>

                        [$]\\varepsilon = E_{d_m} - E_1 =
                        \\frac{1}{2N_h}\\sum_p \\sum_i \\sum_k (y_{i, -k}-d_i)^2 - \\frac{1}{2}\\sum_p \\sum_i (y_i-d_i)^2[/$]:
                                chyba když zmizí průměrný skrytý neuron<br>

                        Odchylka výstupu ke ztrátě neuronu: [$]E_{y_m}=\\frac{1}{2N_h}\\sum_p\\sum_i\\sum_k(y_i-y_{i,-k})^2[/$]<br>

                        Nejhorší následky: [$]E_d=\\max_{p,i,k}\\|y_{i,-k}-d_i\\|[/$]
                        [$]E_y=\\max_{p,i,k}\\|y_{i,-k}-y_i\\|[/$]
        -
                uuid: aed8cb4d-e1b2-4fb3-ba9d-e2eded605006
                front: Trik při minimalizaci robustnostního členu [$]E_{y_m}[/$]
                back: >
                        Minimalizuj místo toho
                        [$]E_{y'_m}=\\frac{1}{N_h}\\sum_p\\sum_i\\sum_k(\\xi_i-\\xi_{i,-k})^2=
                        \\frac{1}{2N_h}\\sum_p\\sum_i\\sum_j(x_j w_{ij})^2[/$]
                        ([$]\\xi_i[/$] místo [$]y_i[/$])
        -
                uuid: 69dbe90a-05a4-4d94-a7c5-99897c231ad0
                front: Prořezávání a duplikace
                back: >
                        Adaptuj síť přes backprop.
                        Identifikuj neuron, jehož ztráta by způsobila největší změnu robustnosti.
                        "Zdvoj" ho.
                        "Rozpůl" váhy obou "kopií" vedoucí k výstupní vrstvě.
                        "Kopie" nahradí některý z ostatních skrytých neuronů.<br>

                        Výsledek jsou robustnější sítě, malá výpočetní složitost, lepší generalizace, omezení problémů s lokálními minimy.
        -
                uuid: 9cd55fa3-f1ae-4229-8446-246c8b2bd259
                front: Pointa modularity
                back: >
                        Rozdělení úlohy do podúloh.<br>

                        Extrahujeme [$]\\varepsilon[/$]-ekvivalentní moduly BP-sítí.<br>
                        Eliminujeme přebytečné skryté/výstupní neurony.<br>
                        Uděláme kompromis mezi přesností extrahovaného modulu a jeho optimální architekturou.
        -
                uuid: ad1ba955-bf80-4f15-b4a9-7b2fd5d7d601
                front: Co je to modul BP-sítě?
                back: >
                        Stejný počet vrstev, každá skrytá vrstva je podmnožina skryté vrstvy hlavní sítě.
        -
                uuid: f7ccbfeb-28b8-4bc1-af0d-5bec78c716e1
                front: Věta o extrakci modulů a její důkaz
                back: >
                        TODO
                        Nechť [$]S[/$] je konečná množina vstupních vzorů,
                        [$]B[/$] BP-síť, [$]H_l[/$] neurony poslední skryté vrstvy.

                        Potom pro libovolné [$]\\varepsilon>1[/$] existuje
                        konstanta [$]K_\\varepsilon>0[/$], že:

                        Pokud existuje v [$]\\ell[/$]-té skryté vrstvě neuron
                        [$]k[/$] takový, že [$] (\\a x\\in S)(\\a i \\in O) |w_ki z_k| \\leq K_\\varepsilon[/$],

                        <br>
                        potom je [$]M^k[/$], ve které odeberu neuron [$]k[/$],
                        [$]\\varepsilon[/$]-ekvivalentní modul [$]B[/$] ([$]M_k\\sim_\\varepsilon^S B[/$]).
        -
                uuid: e650e8f0-1358-455a-9c08-1483092e9174
                front: Co chceme od NS s modulární architekturou?
                back: >
                        Rychlost a konvergenci učení.<br>
                        Optimalizaci architektury.<br>
                        Robustnost a generalizaci.<br>
                        "Useful diversity" jednotlivých modulů.
        -
                uuid: b2c2aa65-8c5d-46a5-952a-780776e99436
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Architektura
                back: >
                        Soustava lokálních sítí s řídící sítí.<br>
                        Lokální jsou BP. Všechny mají stejnou topologii.<br>
                        Řídící síť má výstupy [$]P_j[/$], stejné vstupy jako lokální sítě.<br>
                        Výstup je [$]\\sum_j P_j y^{(j)}[/$].
        -
                uuid: 4166fe03-4498-4ba3-98d7-323484d30059
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Co by mělo platit pro koeficienty [$]p_i[/$]?
                back: >
                        [$$]p_i=\\frac{y_i^{(g)}}{\\sum_{j=1}^t y_j^{(g)}}, i=1\\ldots t, p_i\\in(0;1), \\sum_i^t p_i = 1[/$$]
        -
                uuid: 2ba0a009-b724-44ee-ad28-ae103ea96678
                topic: Adaptivní směsi lokálních neuronových sítí
                front: Chybová funkce
                back: >
                        Pro jeden vzor [$]p[/$]:
                        [$$]E_p=\\frac{1}{2}\\sum_{i=1}^t (p_i\\|y^{(i)}-d\\|)^2[/$$]<br>

                        Plná chybová funkce je suma přes všechny vzory.<br>

                        Kde: [$]p_i[/$] je gating koeficient pro danou síť,
                        [$]y^{(i)}[/$] je výstup jedné sítě, [$]t[/$] je počet
                        síťových modulů.<br>

                        Vyjádří se jako:
                        [$$]E_p = \\frac{1}{2}\\sum_{i=1}^t (\\sum_k (g_{k}^{(i)})^2)[/$$]
                        Kde [$]g_k^{(i)}=p_i\\|y_k^{(i)}-d_k\\|[/$] pro výstupní neuron, jinak [$]0[/$]
        -
                uuid: 770144db-51f5-4b29-a6ce-7b90ddd960b9
                front: Síť na inverzní kinematiku robota
                back: >
                        Kontextová síť: na základě kontextového vstupu určuje váhy pro funkční síť.<br>
                        Převedení: zobrazení [$]n\\rightarrow n^2[/$] na [$]n^2[/$] funkcí [$]n\\rightarrow 1[/$]

